# ETRM Overfit Phase Experiments
# Project: etrm-overfit
# Goal: Find configurations that can overfit to 32 groups (>90% train accuracy)
# See: docs/experiments/etrm_overfit_experiments.md

# Common settings for all experiments:
# - 32 train groups, 32 eval groups (small subset)
# - Pretrained decoder (unless testing from scratch)
# - 20,000 epochs max, eval every 1,000 steps
# - grad_clip_norm=1.0

# =============================================================================
# PHASE 1: Baseline Validation & Re-test Failed Encoders (Week 1)
# =============================================================================

# E1a: Baseline Standard Encoder (previously at 86% step 240, restarting)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.5 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E1a_baseline"

# E2a: Hybrid Variational Encoder (previously 50.6% with cached ETRM)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 arch.halt_exploration_prob=0.5 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E2a_variational"

# E2b: LPN Variational Encoder (previously 51.5% with cached ETRM)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=lpn_variational arch.encoder_num_layers=6 arch.encoder_norm_style=pre arch.encoder_set_layers=3 arch.halt_exploration_prob=0.5 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E2b_lpn_variational"

# E2c: Hybrid Standard Encoder (deeper, no VAE)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.encoder_norm_style=pre arch.encoder_set_layers=2 arch.halt_exploration_prob=0.5 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E2c_hybrid_standard"

# =============================================================================
# PHASE 2: Halting Dynamics Exploration (Week 2)
# =============================================================================

# E1b: Lower exploration (0.3 vs 0.5 baseline)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.3 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E1b_explore0.3"

# E1c: No exploration (0.0 - pure Q-head learning)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.0 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E1c_explore0.0"

# E1d: Higher exploration (0.7)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_exploration_prob=0.7 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E1d_explore0.7"

# E3a: Shorter max steps (8 vs 16 baseline)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_max_steps=8 arch.halt_exploration_prob=0.5 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E3a_maxsteps8"

# E3b: Longer max steps (32 vs 16 baseline)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 arch.halt_max_steps=32 arch.halt_exploration_prob=0.5 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=1000 +project_name="etrm-overfit" +run_name="E3b_maxsteps32"

# =============================================================================
# NOTES
# =============================================================================
#
# Priority order:
# 1. PHASE 1 (Week 1): E2a, E2b, E2c - Re-test failed encoders with re-encoding
# 2. PHASE 2 (Week 2): E1b, E1c, E1d, E3a, E3b - Understand halting dynamics
#
# Success criteria:
# - Train accuracy >90% within 5000 steps
# - Train exact match >50%
# - Q-halt accuracy >85%
# - Avg steps < halt_max_steps (showing dynamic halting works)
#
# Failure modes to watch:
# - Stuck at 50-60% → Check encoder gradients, may be architecture issue
# - Stuck at 70-80% → May need more steps or hyperparameter tuning
# - Diverges → Gradient clipping or learning rate issue
# - Very slow → Performance bug, check encoder efficiency
#
# After completion:
# - Document results in docs/experiments/etrm_overfit_results.md
# - Select top 2-3 configurations for full dataset experiments
# - Update docs/future_experiments.md with findings