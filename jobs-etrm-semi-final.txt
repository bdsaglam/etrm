# ETRM Semi-Final Experiments - Full Dataset Exploration
# Goal: Identify best architecture + hyperparams for final 50k epoch training
#
# Dataset: Full training set (~560 puzzle groups)
# Duration: 1000 epochs per experiment (quick test to identify best configs)
# Eval: Every 200 epochs (5 evaluation points per run)
# Note: Shorter than planned 10k steps for faster iteration
#
# Architectures tested:
# 1. hybrid_standard (4-layer): Expected winner - high variance + healthy gradients
# 2. hybrid_variational (4-layer): Cross-attention architecture, deterministic (kl=0)
# 3. standard (2-layer): Baseline comparison (E1a architecture)
#
# Hyperparams tested for each:
# - TRM paper baseline: max_steps=16, explore=0.5
# - Best empirical: max_steps=16, explore=0.7 (healthy gradients from overfit)
#
# Expected outcome: Identify 1-2 configs for final long training (50k epochs)

# =============================================================================
# Hybrid Standard (4-layer) - PRIMARY RECOMMENDATION
# Expected: 35-45% test EM based on high variance (1.70) + healthy gradients (0.26)
# =============================================================================

# SF1: Hybrid_standard with TRM paper baseline hyperparams
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF1_hybrid_std_baseline" arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=128 epochs=1000 eval_interval=200 [2697533]

# SF2: Hybrid_standard with high exploration (best from overfit with healthy grads)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF2_hybrid_std_explore0.7" arch.encoder_type=hybrid_standard arch.encoder_num_layers=4 arch.halt_max_steps=16 arch.halt_exploration_prob=0.7 global_batch_size=128 epochs=1000 eval_interval=200

# =============================================================================
# Hybrid Variational (4-layer) - CROSS-ATTENTION ARCHITECTURE
# Rationale: Test cross-attention architecture (sees all demos jointly)
# Uses kl_weight=0 for deterministic training (avoids stochastic sampling issues)
# Expected: Similar to hybrid_standard but with attention-based aggregation
# =============================================================================

# SF3: Hybrid_variational with TRM paper baseline (deterministic, kl=0)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF3_hybrid_var_baseline" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.loss.kl_weight=0.0 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=128 epochs=1000 eval_interval=200

# SF4: Hybrid_variational with high exploration (deterministic, kl=0)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF4_hybrid_var_explore0.7" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.loss.kl_weight=0.0 arch.halt_max_steps=16 arch.halt_exploration_prob=0.7 global_batch_size=128 epochs=1000 eval_interval=200

# =============================================================================
# BASELINE COMPARISON - Standard 2-layer (for reference)
# =============================================================================

# SF5: Standard 2-layer with paper baseline (control - what we know works)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF5_standard_baseline" arch.encoder_type=standard arch.encoder_num_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=256 epochs=1000 eval_interval=200

# SF6: Standard 2-layer with high exploration (test if 0.7 helps simple encoder)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="etrm-semi-final" +run_name="SF6_standard_explore0.7" arch.encoder_type=standard arch.encoder_num_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.7 global_batch_size=256 epochs=1000 eval_interval=200

# =============================================================================
# NOTES FOR NEXT PHASE
# =============================================================================
#
# After these experiments complete (est. ~12-24 hours for all):
# 1. Run notebook analysis (notebooks/etrm_experiment_analysis.ipynb with PROJECT="etrm-semi-final")
# 2. Look for:
#    - Test set accuracy (ARC/pass@1) - PRIMARY METRIC
#    - Healthy gradient flow (grad/encoder_norm > 0.2)
#    - Learning curves still improving at 10k steps
#    - Train/test gap (overfitting indicators)
# 3. Select top 1-2 configs for final training:
#    - 50k steps (5x longer, full convergence)
#    - Same hyperparams as semi-final winner(s)
#    - Expected final performance: 35-50% test EM (competitive with TRM's 45% but with true generalization)
#
# Key Questions to Answer:
# - Does hybrid_standard's high variance translate to better test accuracy?
# - Does hybrid_variational's cross-attention improve over hybrid_standard's mean aggregation?
# - Is explore=0.7 still better than 0.5 on full dataset?
# - What's the train/test gap for each architecture?
# - How does deterministic variational (kl=0) compare to standard encoder?
