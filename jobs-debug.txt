# ============================================================================
# OVERFIT EXPERIMENTS: Comparing Training Dynamics Across Approaches
#
# Goal: Understand how each approach learns by overfitting on a small dataset
# Dataset: 8 training groups (very small for quick overfitting)
# Expected: All should reach 95%+ train accuracy if learning dynamics work
# ============================================================================
# ----------------------------------------------------------------------------
# Approach 2: Online TRM with Encoder (Our Baseline)
# ----------------------------------------------------------------------------
# Uses pretrain_encoder.py with online learning (num_act_steps=4)
# Training: 4 forwards per batch, encoder gradients from all steps
# Expected: Should overfit perfectly (~100% accuracy)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=8 max_eval_groups=8 epochs=2000 eval_interval=500 lr=1e-4 +project_name="mmi-714-overfit" +run_name="A2_online"

# ----------------------------------------------------------------------------
# Approach 4: ACT TRM with Re-encoding (New Implementation)
# ----------------------------------------------------------------------------
# Uses pretrain_encoder_original.py with re-encoding (no caching)
# Training: 1 forward per batch, carry persists, full encoder gradients
# Expected: Should overfit like A1, but might need more batches (4x)
# Note: May need higher LR or more epochs since fewer gradient updates per batch
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=8 max_eval_groups=8 epochs=8000 eval_interval=500 lr=2e-4 +project_name="mmi-714-overfit" +run_name="A4_reencode"

# ----------------------------------------------------------------------------
# ----------------------------------------------------------------------------
# Approach 4 FIXED: Critical bug fix for label mismatch
# ----------------------------------------------------------------------------
# Bug: Loss was computed using batch["labels"] instead of actual labels
# For continuing samples, this meant wrong input-label pairs (puzzle A input vs puzzle B labels)
# Fix: Model now returns actual labels used in outputs["labels"]
# Expected: Should now overfit properly like A2
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=8 max_eval_groups=8 epochs=8000 eval_interval=500 lr=2e-4 +project_name="mmi-714-overfit" +run_name="A4_reencode_FIXED"

# ----------------------------------------------------------------------------
# Success Criteria:
# ----------------------------------------------------------------------------
# 1. A1 (embeddings): Should reach ~100% train accuracy quickly (gold standard)
# 2. A2 (online): ✓ 68% at step 60 with pretrained decoder
#    - A2_online_scratch: Should match A1 (both train from scratch)
# 3. A4 (reencode): Was stuck at 42% due to label mismatch bug
#    - A4_reencode_FIXED: Should now overfit properly ✓ Looking good!
#    - A4_reencode_scratch: Should match A1 and A2_scratch (from scratch)
#
# Fair Comparison Groups:
# - FROM SCRATCH: A1_embeddings vs A2_online_scratch vs A4_reencode_scratch
#   → All start from random init, direct comparison of training dynamics
# - WITH PRETRAIN: A2_online vs A4_reencode_FIXED
#   → Both use pretrained decoder, compare encoder training approaches
#
# Key Metrics to Track:
# - train/accuracy: Should reach ~100% for all approaches
# - train/steps: A4 should show adaptive halting (varying steps)
# - grad/encoder_norm: A2 and A4 should both show encoder gradients
# - train/lm_loss: Should decrease to near 0 for all approaches

# ----------------------------------------------------------------------------
# Approach 1: Original TRM with Learned Embeddings (Paper Implementation)
# ----------------------------------------------------------------------------
# Uses pretrain.py with puzzle_emb matrix
# Training: ACT with dynamic halting, embeddings are learned from scratch
# Expected: Should overfit perfectly (~100% accuracy)
# Note: No pretraining - learns embeddings + TRM together
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py --config-name cfg_pretrain_arc_agi_1 max_train_groups=8 max_eval_groups=8 epochs=2000 eval_interval=500 lr=1e-4 +project_name="mmi-714-overfit" +run_name="A1_embeddings"

# ----------------------------------------------------------------------------
# Approach 2 FROM SCRATCH: Online mode without pretrained decoder
# ----------------------------------------------------------------------------
# Same as A2 but trains encoder + TRM decoder from scratch (like A1)
# Expected: Should match A1 (both train from scratch)
# This gives fair comparison: A1 vs A2 vs A4 all starting from random init
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 max_train_groups=8 max_eval_groups=8 epochs=2000 eval_interval=500 lr=1e-4 +project_name="mmi-714-overfit" +run_name="A2_online_scratch" [2142234]

# ----------------------------------------------------------------------------
# Approach 4 FROM SCRATCH: ACT TRM with re-encoding, no pretrained decoder
# ----------------------------------------------------------------------------
# Same as A4_reencode_FIXED but trains encoder + TRM decoder from scratch
# Expected: Should match A1 and A2_scratch (all from scratch)
# Note: Still using 4x epochs and 2x LR due to fewer gradient updates per batch
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 max_train_groups=8 max_eval_groups=8 epochs=8000 eval_interval=500 lr=2e-4 +project_name="mmi-714-overfit" +run_name="A4_reencode_scratch"
