# TEST CONFIG: Frozen Decoder Experiment (CORRECTED HYPERPARAMETERS)
#
# Purpose: Test if encoder can learn with frozen pretrained decoder
# This isolates whether gradients flow correctly to the encoder
#
# CRITICAL FIX: Uses halt_exploration_prob=0.1 (was 0.5)
# CRITICAL FIX: Uses L_cycles=4 to match pretrained decoder training
#
# Expected outcome:
# - If encoder learns (loss decreases), gradients are flowing ✅
# - If encoder doesn't learn (loss flat), there's a gradient blocking bug ❌

defaults:
  - cfg_pretrain_etrm_arc_agi_1
  - _self_

# Override: Load official TRM checkpoint and freeze decoder
load_pretrained_decoder: ./checkpoints/official_trm/arc_v1_public/step_518071
freeze_decoder_steps: 999999  # Freeze forever (or set to total steps)

# CRITICAL: Match TRM hyperparameters
arch:
  L_cycles: 4  # FIXED: Match pretrained decoder training (was 6)

# Use separate optimizer for encoder (so we can see its learning rate)
encoder_lr_scale: 1.0  # Same LR as decoder would have (but decoder is frozen)

# Overfit test parameters
global_batch_size: 256
epochs: 20000
eval_interval: 10000
max_train_groups: 32
max_eval_groups: 4
lr: 3e-4
lr_warmup_steps: 1000
log_predictions_every: 2000

# Project naming
project_name: "etrm-overfit"
