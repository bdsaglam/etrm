# ETRMTRM training config - Recurrent Encoder + TRM Decoder
#
# Uses recurrent encoder that has carry state:
# - Encoder called every ACT step with carry
# - Context evolves across steps
# - Decoder same as ETRM

defaults:
  - arch: etrmtrm
  - _self_

hydra:
  output_subdir: null

# Override arch values
arch:
  L_layers: 2
  H_cycles: 3
  L_cycles: 6
  halt_exploration_prob: 0.5  # Controls Q-head exploration during training
  recurrent_encoder_type: recurrent_standard  # Start with simpler Variant A

# Data path - use encoder-mode preprocessed data
data_paths: ["data/arc1concept-encoder-aug-1000"]
data_paths_test: []
max_train_groups: null
max_eval_groups: null

evaluators:
  - name: arc@ARC

# Prediction visualization logging
log_predictions_every: 5000
log_predictions_max_samples: 32
log_predictions_crop: True

# Hyperparams - Training
global_batch_size: 256
max_demos: 10

epochs: 100000
eval_interval: 10000
checkpoint_every_eval: True

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM
beta1: 0.9
beta2: 0.95
weight_decay: 0.1

# Gradient clipping (essential for stability)
grad_clip_norm: 1.0

# Separate encoder learning rate (multiplier)
# 0 = disabled (single optimizer), 0.1 = encoder 10x slower
encoder_lr_scale: 0.0

# Transfer learning: load pretrained TRM decoder weights
load_pretrained_decoder: null

# Staged training: freeze decoder for first N steps (train encoder only)
freeze_decoder_steps: 0

seed: 0
min_eval_interval: 0

ema: True
ema_rate: 0.999

# Not used in encoder mode, kept for compatibility
freeze_weights: False
