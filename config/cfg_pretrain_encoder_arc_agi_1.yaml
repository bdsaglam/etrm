# Encoder-based TRM training config

defaults:
  - arch: trm_encoder
  - _self_

hydra:
  output_subdir: null

# Override arch values
arch:
  L_layers: 2
  H_cycles: 3
  L_cycles: 6

# Data path - use encoder-mode preprocessed data
data_paths: ["data/arc1concept-encoder-aug-1000"]
data_paths_test: []
max_train_groups: null
max_eval_groups: null

evaluators:
  - name: arc@ARC

# Prediction visualization logging
log_predictions_every: 5000
log_predictions_max_samples: 32  # Max samples per log
log_predictions_crop: True  # If true, crop to content; if false, show full 30x30

# Hyperparams - Training
global_batch_size: 256
max_demos: 10  # Maximum demos per batch item

epochs: 100000
eval_interval: 10000
checkpoint_every_eval: True

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM
beta1: 0.9
beta2: 0.95
weight_decay: 0.1

# Gradient clipping (0 = disabled)
# Note: 1.0 significantly improves training stability (96.7% vs 85.4% on overfit test)
grad_clip_norm: 1.0

# Separate encoder learning rate (multiplier)
# 0 = disabled (single optimizer), 0.1 = encoder 10x slower
encoder_lr_scale: 0.0

# Transfer learning: load pretrained TRM decoder weights
# null = disabled, path = load from checkpoint (skips puzzle_emb)
load_pretrained_decoder: null

# Staged training: freeze decoder for first N steps (train encoder only)
# 0 = disabled (train full model from start)
freeze_decoder_steps: 0

# NO puzzle_emb_lr - single optimizer for encoder mode

seed: 0
min_eval_interval: 0

ema: True
ema_rate: 0.999

# Not used in encoder mode, kept for compatibility
freeze_weights: False
