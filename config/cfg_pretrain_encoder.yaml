# Encoder-based TRM training config

defaults:
  - arch: trm_encoder
  - _self_

hydra:
  output_subdir: null

# Data path - use encoder-mode preprocessed data
data_paths: ['data/arc1concept-encoder-aug-1000']
data_paths_test: []

evaluators:
  - name: arc@ARC

# Prediction visualization logging
log_predictions_every: null
log_predictions_max_samples: 32
log_predictions_crop: true

# Hyperparams - Training
global_batch_size: 256
max_demos: 10  # Maximum demos per batch item

epochs: 100000
eval_interval: 10000
checkpoint_every_eval: True

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM
beta1: 0.9
beta2: 0.95
weight_decay: 0.1

# NO puzzle_emb_lr - single optimizer for encoder mode

seed: 0
min_eval_interval: 0

ema: False
ema_rate: 0.999

# Not used in encoder mode, kept for compatibility
freeze_weights: False
max_train_puzzles: null
