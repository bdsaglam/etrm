# ETRMTRM Overfit Experiments
# See docs/experiments/etrmtrm_overfit_experiments.md for experiment design

# ============================================================================
# WEEK 1: VALIDATION (Priority 1)
# Goal: Verify ETRMTRM works and doesn't regress from ETRM
# ============================================================================

# E0b: ETRMTRM Baseline - CRITICAL VALIDATION
# This is the most important experiment - must pass before proceeding
# Expected: >90% train acc, similar convergence to ETRM
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_pretrain_etrmtrm_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=10000 arch.recurrent_encoder_type=recurrent_standard arch.encoder_num_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=256 +project_name="etrmtrm-overfit" +run_name="E0b_etrmtrm_baseline" 

# E0b_ema: ETRMTRM Baseline with EMA Smoothing - FIX FOR SPIKES
# Same as E0b but with EMA smoothing (Fix 2) to stabilize encoder updates
# Fix implemented in recurrent_standard.py:212-218
# Expected: Smoother metrics, reduced spikes, same convergence as E0b
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_pretrain_etrmtrm_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=10000 arch.recurrent_encoder_type=recurrent_standard arch.encoder_num_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=256 +project_name="etrmtrm-overfit" +run_name="E0b_ema_smoothing" [3145981]

# E0c: TRM-style Encoder (Variant B) - TEST HIERARCHICAL REASONING
# Test TRM-style encoder with H/L loops (more complex than Variant A)
# Expected: May need more parameters but could learn richer patterns
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_pretrain_etrmtrm_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=10000 arch.recurrent_encoder_type=trm_style arch.encoder_h_cycles=3 arch.encoder_l_cycles=1 arch.encoder_l_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=256 +project_name="etrmtrm-overfit" +run_name="E0c_trm_style_baseline"

# E0c_simpler: TRM-style with fewer H cycles
# Test if 2 H cycles is sufficient (faster than 3)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_pretrain_etrmtrm_arc_agi_1 load_pretrained_decoder=/home/baris/repos/trm/checkpoints/Arc1concept-aug-1000-ACT-torch/pretrain_att_arc1concept_4/step_518071 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=10000 arch.recurrent_encoder_type=trm_style arch.encoder_h_cycles=2 arch.encoder_l_cycles=1 arch.encoder_l_layers=2 arch.halt_max_steps=16 arch.halt_exploration_prob=0.5 global_batch_size=256 +project_name="etrmtrm-overfit" +run_name="E0c_trm_style_h2"
