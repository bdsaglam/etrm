# ============================================================================
# ONLINE LEARNING ACT IMPLEMENTATION
# Previous bug: ACT halting logic caused encoder to be skipped after first batch
# Previous fix: Force halted=True each batch (works but only 1 ACT step)
#
# New implementation: TRUE ONLINE LEARNING (matches original paper)
# - Each ACT step: forward → backward → optim.step()
# - Encoder re-encodes demos fresh each step (gradients at every step)
# - Later ACT steps benefit from weight updates made by earlier steps
# - This matches original paper's training dynamics exactly
# - Eval still uses adaptive halting with halt_max_steps
#
# Default: num_act_steps=1, grad_clip_norm=1.0
# ============================================================================

# ============================================================================
# ACT STEPS ABLATION (on overfit test - 32 groups)
# Goal: Test impact of different ACT step counts during training
# ============================================================================

# A1: 1 ACT step (baseline - same as previous fix)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="A1_act_steps_1"

# A4: 4 ACT steps
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=4 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A4_act_steps_4"

# A8: 8 ACT steps
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=8 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A8_act_steps_8"

# A16: 16 ACT steps (matches eval halt_max_steps)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.num_act_steps=16 max_train_groups=32 max_eval_groups=32 +project_name="mmi-714-debug" +run_name="A16_act_steps_16"

# ============================================================================
# GENERALIZATION EXPERIMENTS
# Goal: Achieve non-zero pass@K on test set (true generalization)
# Strategy: Train on full dataset (~560 puzzles), eval on held-out (~400 puzzles)
# Uses best ACT step count from ablation above (start with 1)
# ============================================================================

# G2: LPN Variational encoder - full training
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 max_eval_groups=32 arch.encoder_type=lpn_variational arch.num_act_steps=4 eval_interval=10000 +project_name="mmi-714-debug" +run_name="G2_lpn_var_full" 

# G3: LPN Standard encoder - full training
[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 max_eval_groups=32 arch.encoder_type=lpn_standard arch.num_act_steps=4 epochs=10000 eval_interval=1000 +project_name="mmi-714-debug" +run_name="G3_lpn_std_full"  [1549095]

# G1: Standard encoder - full training baseline
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 max_eval_groups=32  arch.num_act_steps=4 epochs=10000 eval_interval=1000 +project_name="mmi-714-debug" +run_name="G1_standard_full"

# ============================================================================
# TRANSFER LEARNING EXPERIMENTS
# Goal: Speed up training by initializing with pretrained TRM decoder
# Strategy: Load pretrained inner model weights, train encoder to produce
#           compatible context vectors
# Pretrained checkpoint: TRM with 45% ARC-AGI-1 accuracy
# ============================================================================

# T1: Pretrained decoder + full training (standard encoder)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/TinyRecursiveModels/checkpoints/trm_ablations_arcagi1/arcagi1_supervision_4/step_25900 arch.num_act_steps=4 eval_interval=5000 +project_name="mmi-714-debug" +run_name="T1_pretrained_standard"

# T2: Pretrained decoder + staged training (encoder-only first 5k steps)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/TinyRecursiveModels/checkpoints/trm_ablations_arcagi1/arcagi1_supervision_4/step_25900 freeze_decoder_steps=5000 arch.num_act_steps=4 eval_interval=5000 +project_name="mmi-714-debug" +run_name="T2_pretrained_staged"

# T3: Pretrained decoder + LPN variational encoder
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 load_pretrained_decoder=/home/baris/repos/TinyRecursiveModels/checkpoints/trm_ablations_arcagi1/arcagi1_supervision_4/step_25900 arch.encoder_type=lpn_variational arch.num_act_steps=4 eval_interval=5000 +project_name="mmi-714-debug" +run_name="T3_pretrained_lpn_var"
