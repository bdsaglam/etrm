# ============================================================================
# PHASE 1: OVERFIT VERIFICATION (must pass before anything else)
# Goal: Prove both baseline and encoder can memorize 32 puzzles
# ============================================================================

# E0v2: Baseline TRM (puzzle embeddings) - extended training
# Expected: Should reach 90%+ accuracy if model works
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py --config-name cfg_pretrain_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E0v2_baseline_overfit_long"

# E1v2: Standard encoder - extended training (same settings as E0v2 for fair comparison)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E1v2_encoder_overfit_long"

# E_diag1: Frozen encoder diagnostic - can TRM learn with random fixed encoder?
# If YES: encoder output format is correct, training signal is the issue
# If NO: encoder-TRM interface or representation format is broken
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.freeze_encoder=true epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E_diag1_frozen_encoder"

# ============================================================================
# PHASE 2: ENCODER ARCHITECTURE COMPARISON
# Goal: Find the best encoder architecture
# Run these after Phase 1 shows encoder can learn
# ============================================================================

# E2_lpn: LPN-style encoder (deeper, CLS token, mean aggregation)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_standard epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E2_lpn_standard_overfit"

# E3_lpn_var: LPN Variational encoder (per-demo VAE)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_type=lpn_variational epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E3_lpn_variational_overfit"

# ============================================================================
# PHASE 3: HYPERPARAMETER TUNING
# Goal: Improve training stability and convergence
# ============================================================================

# E4_lower_lr: Standard encoder with lower learning rate
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E4_encoder_lr3e5"

# E5_grad_clip: Standard encoder with gradient clipping
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=1.0 epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E5_encoder_gradclip1"

# E7_attention: Attention pooling only (isolate this variable from E6)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_pooling_method=attention epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E7_attention_pooling"

# E6_combined: Lower LR + grad clip + attention pooling
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 grad_clip_norm=1.0 arch.encoder_pooling_method=attention epochs=100000 max_train_groups=32 max_eval_groups=32 global_batch_size=256 eval_interval=10000 log_predictions_every=5000 +project_name="mmi-714-debug" +run_name="E6_encoder_combined"
