# ETRM Staged Training Experiments
# Goal: Test staged training (frozen decoder → joint training) on full dataset
#
# Strategy:
# - Phase 1 (0-2500 steps): Train encoder only, decoder frozen at pretrained weights
# - Phase 2 (2500-10k epochs): Unfreeze decoder, train jointly
#
# Hypothesis:
# - Encoder learns better task representations when trained alone first
# - Joint training can then fine-tune both encoder and decoder together
# - This may prevent encoder from being overwhelmed by decoder gradients
#
# Dataset:
#   - Train: Full training set (~560 puzzle groups)
#   - Eval: 32 puzzle groups from test set (for speed)
# Duration: 10k epochs per experiment (~12-18 hours on 4 GPUs)
# Eval: Every 5k epochs (2 evaluation points)
#
# Success criteria:
# - Test set ARC/pass@1 should improve over overfit experiments
# - Phase 1: Encoder should learn (train loss decreases)
# - Phase 2: Joint training should improve test accuracy further
#
# Architectures to test:
# 1. standard (2-layer): Simple baseline
# 2. hybrid_variational (4-layer): Best performer from overfit tests
#
# =============================================================================
# S1: STANDARD ENCODER (2-layer) - BASELINE
# =============================================================================
# Simple architecture, good gradient flow
# Staged training: Freeze decoder for 2500 steps, then joint train

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S1_standard"

# =============================================================================
# S2: HYBRID VARIATIONAL (4-layer) - BEST FROM OVERFIT
# =============================================================================
# Cross-attention architecture with variational bottleneck
# Expected to perform best based on overfit experiments

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S2_hybrid_var" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 global_batch_size=128

# =============================================================================
# S3: ETRMTRM TRM-STYLE ENCODER
# =============================================================================
# Recurrent encoder with H/L loops matching decoder architecture
# Most complex architecture - tests if recurrence helps

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_staged_training +run_name="S3_etrmtrm" arch.recurrent_encoder_type=trm_style arch.encoder_h_cycles=3 arch.encoder_l_cycles=4 global_batch_size=128

# =============================================================================
# S4: LPN PAPER-EXACT VARIATIONAL
# =============================================================================
# Smallest encoder (673K params)
# Tests if compact architecture can still learn with staged training

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S4_lpn_var" arch.encoder_type=lpn_var

# =============================================================================
# WHAT TO WATCH (Real-time in WandB)
# =============================================================================
#
# Phase 1 Metrics (Steps 0-2500, Decoder Frozen):
# 1. train/decoder_frozen: Should be 1
# 2. grad/encoder_norm: 0.1-1.0 (encoder learning)
# 3. grad/inner_norm: 0 (decoder frozen)
# 4. train/lm_loss: Should decrease (encoder learning)
# 5. train/encoder_token_std: Should increase (diversity)
#
# Phase 2 Metrics (Steps 2500+, Joint Training):
# 6. train/decoder_frozen: Should become 0
# 7. grad/encoder_norm: Should remain stable
# 8. grad/inner_norm: Should become >0 (decoder learning)
# 9. train/lm_loss: Should continue decreasing
# 10. ARC/pass@1: Should improve (critical!)
#
# Key Comparison:
# - Compare to overfit experiments (O1-O4)
# - Full dataset should → better generalization
# - Staged training should → higher test accuracy
#
# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# Scenario A: Staged Training Helps ✅
#   Phase 1: Encoder learns task representations (train loss decreases)
#   Phase 2: Joint training improves further
#   Test accuracy: 10-20% (significant improvement over overfit 5-15%)
#   → Conclusion: Staged training prevents encoder from being overwhelmed
#
# Scenario B: Joint Training Hurts ❌
#   Phase 1: Encoder learns well
#   Phase 2: Test accuracy drops when decoder unfreezes
#   → Conclusion: Decoder gradients interfere with encoder learning
#   → Action: Try longer frozen phase or slower encoder LR
#
# Scenario C: No Improvement Over Overfit ⚠️
#   Test accuracy: Similar to overfit experiments (5-15%)
#   → Conclusion: Full dataset doesn't help, or 10k epochs not enough
#   → Action: Try longer training (50k epochs) or stronger encoder
#
# =============================================================================
# VERIFICATION COMMANDS
# =============================================================================
#
# After training, check decoder weights diverged from pretrained:
# for run in S1_standard S2_hybrid_var S3_etrmtrm S4_lpn_var; do
#   python scripts/verify_decoder_loading.py \
#     --trm-checkpoint ./checkpoints/official_trm/arc_v1_public/step_518071 \
#     --etrm-checkpoint ./checkpoints/etrm-staged/$run/step_10000
# done
#
# Expected: Decoder weights should NOT match 100% (decoder trained in phase 2)
#
# Compare metrics:
# - Overfit (O1-O4): 32 groups, 20k epochs, frozen decoder
# - Staged (S1-S4): Full dataset, 10k epochs, staged training
#
# =============================================================================
# NEXT STEPS AFTER RESULTS
# =============================================================================
#
# If Scenario A (Staged Training Helps):
#   1. Great! Staged training improves performance
#   2. Run longer training (50k epochs) on best architecture
#   3. Try different freeze durations (1k, 5k, 10k steps)
#   4. Expected final: 25-35% test accuracy
#
# If Scenario B (Joint Training Hurts):
#   1. Keep decoder frozen permanently (best from overfit experiments)
#   2. Or try encoder_lr_scale=0.1 (slower encoder in phase 2)
#   3. Or try longer freeze phase (5k-10k steps)
#
# If Scenario C (No Improvement):
#   1. Run 50k epochs (may need more training)
#   2. Try stronger encoder architecture
#   3. Investigate why full dataset doesn't help
#
# =============================================================================
# NOTES
# =============================================================================
#
# Key differences from overfit experiments:
# - Dataset: Full training set (~560 groups) vs 32 groups
# - Duration: 10k epochs vs 20k epochs
# - Decoder: Unfreezes after 2500 steps vs stays frozen
# - Learning rate: 1e-4 (standard) vs 3e-4 (fast overfit)
# - Expected: Better generalization due to more data
#
# Staged training rationale:
# - Encoder needs time to learn task representations first
# - Joint training from start may overwhelm encoder with decoder gradients
# - Staged approach gives encoder a "head start"
#
# Timeline: ~12-18 hours per experiment on 4 GPUs
# Total: ~48-72 hours for all 4 architectures
#
