[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 log_predictions_every=100 epochs=1000 eval_interval=100 +project_name="mmi-714" +run_name="encoder_trm_v1"
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 log_predictions_every=100 epochs=1000 eval_interval=100 +project_name="mmi-714" +run_name="encoder_trm_diagnose"
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py --config-name cfg_pretrain_arc_agi_1 log_predictions_every=1000 +project_name="mmi-714" +run_name="trm_repro"

# === Encoder Architecture Verification Experiments ===

# E1: Overfit test - CAN the model memorize 32 samples? (MUST PASS FIRST)
[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 epochs=1000 max_train_puzzles=32 eval_interval=100 log_predictions_every=100 +project_name="mmi-714" +run_name="E1_overfit_32samples"

# E2: Lower learning rate for stability
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E2_lower_lr_3e5" [1121504]

# E3: Gradient clipping only
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 grad_clip_norm=1.0 epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E3_grad_clip_1.0"

# E4: Lower LR + gradient clipping (combined stability)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 grad_clip_norm=1.0 epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E4_lower_lr_grad_clip"

# E7: Attention pooling instead of mean pooling
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_pooling_method=attention epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E7_attention_pooling"

# E11: Separate learning rates (encoder 10x slower)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 encoder_lr_scale=0.1 epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E11_encoder_lr_0.1x"

# E_combined: Best practices together (attention pooling + lower LR + grad clip + layer scale + pre-norm)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 lr=3e-5 grad_clip_norm=1.0 arch.encoder_pooling_method=attention arch.encoder_layer_scale_init=1e-4 arch.encoder_norm_style=pre epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E_combined_best_practices"

# E_deeper: Deeper encoder (4 layers grid + 2 layers set)
[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder.py --config-name cfg_pretrain_encoder_arc_agi_1 arch.encoder_num_layers=4 arch.encoder_set_layers=2 epochs=2000 eval_interval=200 log_predictions_every=100 +project_name="mmi-714" +run_name="E_deeper_4L_2S"