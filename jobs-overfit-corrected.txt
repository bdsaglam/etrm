# ETRM Overfit Experiments - CORRECTED HYPERPARAMETERS
# Goal: Quick overfit tests (32 train groups) to validate architectures
#
# CRITICAL FIXES APPLIED:
# 1. halt_exploration_prob: 0.5 â†’ 0.1 (match TRM paper)
# 2. L_cycles: 6 â†’ 4 (match pretrained decoder training)
# 3. All shared hyperparameters aligned with official TRM
#
# Background:
# - Previous experiments used WRONG hyperparameters (5x too much exploration)
# - First corrected experiment showed NON-ZERO test metrics (breakthrough!)
# - Now testing all 4 architectures with correct config
#
# Dataset:
#   - Train: 32 puzzle groups (quick overfit test)
#   - Eval: 4 puzzle groups from test set
# Duration: 20k epochs per experiment (~2-3 hours on 4 GPUs)
# Eval: Every 10k epochs (2 evaluation points)
#
# Success criteria:
# - Train loss should decrease (encoder learning)
# - train/q_halt_accuracy: >95% (Q-head learning with correct exploration)
# - ARC/pass@1: >0% on test set (critical - proves generalization)
# - Gradient health: grad/encoder_norm in 0.1-1.0 range
#
# Architectures to test:
# 1. standard (2-layer): Simple baseline
# 2. hybrid_variational (4-layer): Cross-attention with KL
# 3. etrmtrm trm_style: Recurrent encoder with H/L loops
# 4. lpn_var: Paper-exact LPN with per-demo VAE
#
# =============================================================================
# O1: STANDARD ENCODER (2-layer) - BASELINE
# =============================================================================
# Simple architecture, good gradient flow
# Best semi-final performer with WRONG hyperparameters (43% train, 1% test)
# Expected with CORRECT hyperparameters: Higher test accuracy

[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O1_standard"

# =============================================================================
# O2: HYBRID VARIATIONAL (4-layer) - CROSS-ATTENTION WITH KL
# =============================================================================
# Cross-attention architecture with variational bottleneck
# KL weight 1e-4 to regularize latent space
# More capacity than standard encoder

[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O2_hybrid_var" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 global_batch_size=128 

# =============================================================================
# O3: ETRMTRM TRM-STYLE ENCODER - RECURRENT WITH H/L LOOPS
# =============================================================================
# TRM-style recurrent encoder (Variant B)
# UPDATED: Match decoder architecture for symmetry
# - encoder_h_cycles: 3 (match decoder H_cycles)
# - encoder_l_cycles: 4 (match decoder L_cycles)
# - encoder_l_layers: 2 (match decoder L_layers)
# This gives encoder same computational depth as decoder

[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O3_etrmtrm" global_batch_size=256

# =============================================================================
# O4a: LPN VARIATIONAL - ORIGINAL (32 dims)
# =============================================================================
# Paper-exact LPN with per-demo VAE: 2 layers, 128 hidden, LayerNorm, SiLU
# latent_dim=32 (original paper value)
# Previously showed LOW cross-sample variance (0.05 vs 0.15 for O1/O3)
# Run with NEW metrics to measure within-puzzle variance

[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O4a_lpn_var_32" arch.encoder_type=lpn_var arch.lpn_latent_dim=32

# =============================================================================
# O4b: LPN VARIATIONAL - INCREASED CAPACITY (512 dims)
# =============================================================================
# LPN encoder with per-demo VAE: 2 layers, 128 hidden, LayerNorm, SiLU
# latent_dim=512 (16Ã— larger) to match other encoders' capacity
# 16 tokens Ã— 512 dims = 8,192 total dims (same as O1/O3)
# This tests if low cross-sample variance was due to 32-dim bottleneck

[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O4b_lpn_var_512" arch.encoder_type=lpn_var arch.lpn_latent_dim=512 [364199]

# =============================================================================
# O4c: LPN VARIATIONAL - 10x KL WEIGHT (512 dims)
# =============================================================================
# Tests if low cross-sample variance is due to kl_weight=0.0001 being too small
# Increased from 0.0001 to 0.001 (10x higher) to enforce stochasticity
# Expected: Higher variance, potentially better diversity if kl_weight was limiting factor

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O4c_lpn_var_512_10xkl" arch.encoder_type=lpn_var arch.lpn_latent_dim=512 arch.kl_weight=0.001

# =============================================================================
# WHAT TO WATCH (Real-time in WandB)
# =============================================================================
#
# Critical Metrics to Compare Across Architectures:
#
# 1. Training Dynamics:
#    - train/lm_loss: Should decrease to <1.0
#    - train/accuracy: Target >70%
#    - train/exact_accuracy: Any improvement is good
#
# 2. Q-head Learning (CRITICAL - measures impact of corrected exploration):
#    - train/q_halt_accuracy: Should be >95% (vs ~97% from previous wrong config)
#    - If significantly higher â†’ confirms exploration_prob fix worked
#
# 3. Gradient Health:
#    - grad/encoder_norm: 0.1-1.0 range (stable)
#    - grad/total_norm: Should not explode
#
# 4. Encoder Output Diversity (NEW METRICS):
#    - train/encoder_token_std: Should increase (>0.3 is good)
#    - train/encoder_cross_sample_var: Should be >0.1
#      â†’ Measures variance across ALL batch samples (mix of different puzzles/augs)
#      â†’ HIGH = encoder learns diverse representations (GOOD!)
#      â†’ LOW = representation collapse (BAD!)
#    - train/encoder_within_group_var: Computed when batch has duplicate puzzle_ids
#      â†’ Measures variance among samples from SAME puzzle (different aug/query)
#      â†’ LOW = encoder robust to augmentations (GOOD!)
#      â†’ HIGH = encoder sensitive to augmentations (may indicate instability)
#
# 5. TEST ACCURACY (MOST IMPORTANT):
#    - ARC/pass@1: MUST be >0% (proves generalization)
#    - Target: 5-15% if architecture is viable
#    - Compare across architectures to find best
#
# 6. Overfitting Gap:
#    - train/accuracy vs ARC/pass@1 gap
#    - Smaller gap = better generalization
#
# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# Scenario A: Hyperparameters Fixed the Issue âœ…
#   All architectures show:
#   - train/q_halt_accuracy: >95%
#   - ARC/pass@1: 5-15% (significant improvement from 0-1% with wrong config)
#   - Architecture ranking: Can now meaningfully compare architectures
#   â†’ Action: Run full training (50k epochs) on best 2 architectures
#
# Scenario B: Some Architectures Work, Others Don't âš ï¸
#   Mixed results:
#   - Some architectures: 5-15% test accuracy
#   - Others: Still 0-1% test accuracy
#   â†’ Action: Investigate why some architectures fail
#   â†’ Hypothesis: Architecture-specific issues (capacity, gradient flow, etc.)
#
# Scenario C: Still Low Performance Across All âŒ
#   All architectures:
#   - train/q_halt_accuracy: >95%
#   - ARC/pass@1: Still 0-1%
#   â†’ Action: Problem is not just hyperparameters
#   â†’ Investigate: Training dynamics, learning rate scheduling, etc.
#
# Scenario D: LPN Variants Comparison (O4a vs O4b vs O4c) ðŸ”¬
#
#   O4a (32 dims, kl=0.0001):
#   - encoder_cross_sample_var: LOW (representation collapse)
#   - encoder_within_group_var: LOW
#   - ARC/pass@1: Baseline performance
#
#   O4b (512 dims, kl=0.0001):
#   - encoder_cross_sample_var: Similar to O4a (capacity not the issue)
#   - encoder_within_group_var: LOW
#   - ARC/pass@1: Similar to O4a
#   â†’ Conclusion: 32â†’512 dims doesn't help, kl_weight is the limiting factor
#
#   O4c (512 dims, kl=0.001 - 10x higher):
#   - encoder_cross_sample_var: Should be HIGHER than O4a/O4b if kl_weight was limiting
#   - encoder_within_group_var: Should be HIGHER (more stochasticity)
#   - ARC/pass@1: Should improve if representation collapse was due to kl_weight
#
#   Interpretation:
#   - If O4c >> O4b: Confirms kl_weight=0.0001 was too low (collapsed to deterministic)
#   - If O4c â‰ˆ O4b: Issue is LPN architecture design, not hyperparameters
#   â†’ Action: If O4c improves, use kl_weight=0.001 for staged training
#
# =============================================================================
# VERIFICATION COMMANDS
# =============================================================================
#
# After all experiments complete, compare results:
#
# 1. Check WandB dashboard:
#    https://wandb.ai/<your-team>/etrm-overfit
#
# 2. Verify decoder stayed frozen for all experiments:
#    for run in O1_standard O2_hybrid_var O3_etrmtrm O4_lpn_var; do
#      python scripts/verify_decoder_loading.py \
#        --trm-checkpoint ./checkpoints/official_trm/arc_v1_public/step_518071 \
#        --etrm-checkpoint ./checkpoints/etrm-overfit/$run/step_10000
#    done
#
# 3. Compare to previous experiments (wrong hyperparameters):
#    - Previous best: F1_standard (43% train, 1% test)
#    - New: Should see higher test accuracy across all architectures
#
# =============================================================================
# NEXT STEPS AFTER RESULTS
# =============================================================================
#
# If results show significant improvement:
#
# 1. Identify best 2 architectures (highest test accuracy)
#
# 2. Update jobs-final.txt with CORRECTED hyperparameters:
#    - Change halt_exploration_prob: 0.5 â†’ 0.1
#    - Change L_cycles: 6 â†’ 4 (when using load_pretrained_decoder)
#    - Keep all other parameters
#
# 3. Run full training (50k epochs, full dataset) on best 2 architectures
#
# 4. Expected final performance:
#    - If overfit test shows 10-15%: Full training might reach 20-30%
#    - Target: Get closer to TRM's 40% baseline
#
# =============================================================================
# NOTES
# =============================================================================
#
# Key differences from previous experiments:
# - halt_exploration_prob: 0.5 â†’ 0.1 (5x less exploration)
# - L_cycles: 6 â†’ 4 (match pretrained decoder)
# - lr: Using 3e-4 for faster convergence in overfit test
# - All other hyperparameters aligned with official TRM checkpoint
#
# Timeline: ~2-3 hours per experiment on 4 GPUs
# Total: ~8-12 hours to complete all 4 architectures
#
# Can run in parallel if you have multiple machines!
#
