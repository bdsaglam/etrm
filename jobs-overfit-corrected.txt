# ETRM Overfit Experiments - CORRECTED HYPERPARAMETERS
# Goal: Quick overfit tests (32 train groups) to validate architectures
#
# CRITICAL FIXES APPLIED:
# 1. halt_exploration_prob: 0.5 → 0.1 (match TRM paper)
# 2. L_cycles: 6 → 4 (match pretrained decoder training)
# 3. All shared hyperparameters aligned with official TRM
#
# Background:
# - Previous experiments used WRONG hyperparameters (5x too much exploration)
# - First corrected experiment showed NON-ZERO test metrics (breakthrough!)
# - Now testing all 4 architectures with correct config
#
# Dataset:
#   - Train: 32 puzzle groups (quick overfit test)
#   - Eval: 4 puzzle groups from test set
# Duration: 20k epochs per experiment (~2-3 hours on 4 GPUs)
# Eval: Every 10k epochs (2 evaluation points)
#
# Success criteria:
# - Train loss should decrease (encoder learning)
# - train/q_halt_accuracy: >95% (Q-head learning with correct exploration)
# - ARC/pass@1: >0% on test set (critical - proves generalization)
# - Gradient health: grad/encoder_norm in 0.1-1.0 range
#
# Architectures to test:
# 1. standard (2-layer): Simple baseline
# 2. hybrid_variational (4-layer): Cross-attention with KL
# 3. etrmtrm trm_style: Recurrent encoder with H/L loops
# 4. lpn_var: Paper-exact LPN with per-demo VAE
#
# =============================================================================
# O1: STANDARD ENCODER (2-layer) - BASELINE
# =============================================================================
# Simple architecture, good gradient flow
# Best semi-final performer with WRONG hyperparameters (43% train, 1% test)
# Expected with CORRECT hyperparameters: Higher test accuracy

[!] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O1_standard"

# =============================================================================
# O2: HYBRID VARIATIONAL (4-layer) - CROSS-ATTENTION WITH KL
# =============================================================================
# Cross-attention architecture with variational bottleneck
# KL weight 1e-4 to regularize latent space
# More capacity than standard encoder

[-] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O2_hybrid_var" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 global_batch_size=128  [160656]

# =============================================================================
# O3: ETRMTRM TRM-STYLE ENCODER - RECURRENT WITH H/L LOOPS
# =============================================================================
# TRM-style recurrent encoder (Variant B)
# UPDATED: Match decoder architecture for symmetry
# - encoder_h_cycles: 3 (match decoder H_cycles)
# - encoder_l_cycles: 4 (match decoder L_cycles)
# - encoder_l_layers: 2 (match decoder L_layers)
# This gives encoder same computational depth as decoder

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O3_etrmtrm" arch.recurrent_encoder_type=trm_style arch.encoder_h_cycles=3 arch.encoder_l_cycles=4 global_batch_size=128

# =============================================================================
# O4: LPN PAPER-EXACT VARIATIONAL
# =============================================================================
# Paper-exact LPN with per-demo VAE: 2 layers, 128 hidden, LayerNorm, SiLU
# 673K params - smallest encoder
# Best LPN semi-final result with WRONG hyperparameters

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_test_frozen_decoder_corrected +run_name="O4_lpn_var" arch.encoder_type=lpn_var

# =============================================================================
# WHAT TO WATCH (Real-time in WandB)
# =============================================================================
#
# Critical Metrics to Compare Across Architectures:
#
# 1. Training Dynamics:
#    - train/lm_loss: Should decrease to <1.0
#    - train/accuracy: Target >70%
#    - train/exact_accuracy: Any improvement is good
#
# 2. Q-head Learning (CRITICAL - measures impact of corrected exploration):
#    - train/q_halt_accuracy: Should be >95% (vs ~97% from previous wrong config)
#    - If significantly higher → confirms exploration_prob fix worked
#
# 3. Gradient Health:
#    - grad/encoder_norm: 0.1-1.0 range (stable)
#    - grad/total_norm: Should not explode
#
# 4. Encoder Output Diversity:
#    - train/encoder_token_std: Should increase (>0.3 is good)
#    - train/encoder_cross_sample_var: Should be >0.1
#
# 5. TEST ACCURACY (MOST IMPORTANT):
#    - ARC/pass@1: MUST be >0% (proves generalization)
#    - Target: 5-15% if architecture is viable
#    - Compare across architectures to find best
#
# 6. Overfitting Gap:
#    - train/accuracy vs ARC/pass@1 gap
#    - Smaller gap = better generalization
#
# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# Scenario A: Hyperparameters Fixed the Issue ✅
#   All architectures show:
#   - train/q_halt_accuracy: >95%
#   - ARC/pass@1: 5-15% (significant improvement from 0-1% with wrong config)
#   - Architecture ranking: Can now meaningfully compare architectures
#   → Action: Run full training (50k epochs) on best 2 architectures
#
# Scenario B: Some Architectures Work, Others Don't ⚠️
#   Mixed results:
#   - Some architectures: 5-15% test accuracy
#   - Others: Still 0-1% test accuracy
#   → Action: Investigate why some architectures fail
#   → Hypothesis: Architecture-specific issues (capacity, gradient flow, etc.)
#
# Scenario C: Still Low Performance Across All ❌
#   All architectures:
#   - train/q_halt_accuracy: >95%
#   - ARC/pass@1: Still 0-1%
#   → Action: Problem is not just hyperparameters
#   → Investigate: Training dynamics, learning rate scheduling, etc.
#
# =============================================================================
# VERIFICATION COMMANDS
# =============================================================================
#
# After all experiments complete, compare results:
#
# 1. Check WandB dashboard:
#    https://wandb.ai/<your-team>/etrm-overfit
#
# 2. Verify decoder stayed frozen for all experiments:
#    for run in O1_standard O2_hybrid_var O3_etrmtrm O4_lpn_var; do
#      python scripts/verify_decoder_loading.py \
#        --trm-checkpoint ./checkpoints/official_trm/arc_v1_public/step_518071 \
#        --etrm-checkpoint ./checkpoints/etrm-overfit/$run/step_10000
#    done
#
# 3. Compare to previous experiments (wrong hyperparameters):
#    - Previous best: F1_standard (43% train, 1% test)
#    - New: Should see higher test accuracy across all architectures
#
# =============================================================================
# NEXT STEPS AFTER RESULTS
# =============================================================================
#
# If results show significant improvement:
#
# 1. Identify best 2 architectures (highest test accuracy)
#
# 2. Update jobs-final.txt with CORRECTED hyperparameters:
#    - Change halt_exploration_prob: 0.5 → 0.1
#    - Change L_cycles: 6 → 4 (when using load_pretrained_decoder)
#    - Keep all other parameters
#
# 3. Run full training (50k epochs, full dataset) on best 2 architectures
#
# 4. Expected final performance:
#    - If overfit test shows 10-15%: Full training might reach 20-30%
#    - Target: Get closer to TRM's 40% baseline
#
# =============================================================================
# NOTES
# =============================================================================
#
# Key differences from previous experiments:
# - halt_exploration_prob: 0.5 → 0.1 (5x less exploration)
# - L_cycles: 6 → 4 (match pretrained decoder)
# - lr: Using 3e-4 for faster convergence in overfit test
# - All other hyperparameters aligned with official TRM checkpoint
#
# Timeline: ~2-3 hours per experiment on 4 GPUs
# Total: ~8-12 hours to complete all 4 architectures
#
# Can run in parallel if you have multiple machines!
#

# ETRM Staged Training Experiments
# Goal: Test staged training (frozen decoder → joint training) on full dataset
#
# Strategy:
# - Phase 1 (0-2500 steps): Train encoder only, decoder frozen at pretrained weights
# - Phase 2 (2500-10k epochs): Unfreeze decoder, train jointly
#
# Hypothesis:
# - Encoder learns better task representations when trained alone first
# - Joint training can then fine-tune both encoder and decoder together
# - This may prevent encoder from being overwhelmed by decoder gradients
#
# Dataset:
#   - Train: Full training set (~560 puzzle groups)
#   - Eval: 32 puzzle groups from test set (for speed)
# Duration: 10k epochs per experiment (~12-18 hours on 4 GPUs)
# Eval: Every 5k epochs (2 evaluation points)
#
# Success criteria:
# - Test set ARC/pass@1 should improve over overfit experiments
# - Phase 1: Encoder should learn (train loss decreases)
# - Phase 2: Joint training should improve test accuracy further
#
# Architectures to test:
# 1. standard (2-layer): Simple baseline
# 2. hybrid_variational (4-layer): Best performer from overfit tests
#
# =============================================================================
# S1: STANDARD ENCODER (2-layer) - BASELINE
# =============================================================================
# Simple architecture, good gradient flow
# Staged training: Freeze decoder for 2500 steps, then joint train

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S1_standard"

# =============================================================================
# S2: HYBRID VARIATIONAL (4-layer) - BEST FROM OVERFIT
# =============================================================================
# Cross-attention architecture with variational bottleneck
# Expected to perform best based on overfit experiments

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S2_hybrid_var" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 global_batch_size=128

# =============================================================================
# S3: ETRMTRM TRM-STYLE ENCODER
# =============================================================================
# Recurrent encoder with H/L loops matching decoder architecture
# Most complex architecture - tests if recurrence helps

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrmtrm.py --config-name cfg_staged_training +run_name="S3_etrmtrm" arch.recurrent_encoder_type=trm_style arch.encoder_h_cycles=3 arch.encoder_l_cycles=4 global_batch_size=128

# =============================================================================
# S4: LPN PAPER-EXACT VARIATIONAL
# =============================================================================
# Smallest encoder (673K params)
# Tests if compact architecture can still learn with staged training

[ ] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_etrm.py --config-name cfg_staged_training +run_name="S4_lpn_var" arch.encoder_type=lpn_var

# =============================================================================
# WHAT TO WATCH (Real-time in WandB)
# =============================================================================
#
# Phase 1 Metrics (Steps 0-2500, Decoder Frozen):
# 1. train/decoder_frozen: Should be 1
# 2. grad/encoder_norm: 0.1-1.0 (encoder learning)
# 3. grad/inner_norm: 0 (decoder frozen)
# 4. train/lm_loss: Should decrease (encoder learning)
# 5. train/encoder_token_std: Should increase (diversity)
#
# Phase 2 Metrics (Steps 2500+, Joint Training):
# 6. train/decoder_frozen: Should become 0
# 7. grad/encoder_norm: Should remain stable
# 8. grad/inner_norm: Should become >0 (decoder learning)
# 9. train/lm_loss: Should continue decreasing
# 10. ARC/pass@1: Should improve (critical!)
#
# Key Comparison:
# - Compare to overfit experiments (O1-O4)
# - Full dataset should → better generalization
# - Staged training should → higher test accuracy
#
# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# Scenario A: Staged Training Helps ✅
#   Phase 1: Encoder learns task representations (train loss decreases)
#   Phase 2: Joint training improves further
#   Test accuracy: 10-20% (significant improvement over overfit 5-15%)
#   → Conclusion: Staged training prevents encoder from being overwhelmed
#
# Scenario B: Joint Training Hurts ❌
#   Phase 1: Encoder learns well
#   Phase 2: Test accuracy drops when decoder unfreezes
#   → Conclusion: Decoder gradients interfere with encoder learning
#   → Action: Try longer frozen phase or slower encoder LR
#
# Scenario C: No Improvement Over Overfit ⚠️
#   Test accuracy: Similar to overfit experiments (5-15%)
#   → Conclusion: Full dataset doesn't help, or 10k epochs not enough
#   → Action: Try longer training (50k epochs) or stronger encoder
#
# =============================================================================
# VERIFICATION COMMANDS
# =============================================================================
#
# After training, check decoder weights diverged from pretrained:
# for run in S1_standard S2_hybrid_var S3_etrmtrm S4_lpn_var; do
#   python scripts/verify_decoder_loading.py \
#     --trm-checkpoint ./checkpoints/official_trm/arc_v1_public/step_518071 \
#     --etrm-checkpoint ./checkpoints/etrm-staged/$run/step_10000
# done
#
# Expected: Decoder weights should NOT match 100% (decoder trained in phase 2)
#
# Compare metrics:
# - Overfit (O1-O4): 32 groups, 20k epochs, frozen decoder
# - Staged (S1-S4): Full dataset, 10k epochs, staged training
#
# =============================================================================
# NEXT STEPS AFTER RESULTS
# =============================================================================
#
# If Scenario A (Staged Training Helps):
#   1. Great! Staged training improves performance
#   2. Run longer training (50k epochs) on best architecture
#   3. Try different freeze durations (1k, 5k, 10k steps)
#   4. Expected final: 25-35% test accuracy
#
# If Scenario B (Joint Training Hurts):
#   1. Keep decoder frozen permanently (best from overfit experiments)
#   2. Or try encoder_lr_scale=0.1 (slower encoder in phase 2)
#   3. Or try longer freeze phase (5k-10k steps)
#
# If Scenario C (No Improvement):
#   1. Run 50k epochs (may need more training)
#   2. Try stronger encoder architecture
#   3. Investigate why full dataset doesn't help
#
# =============================================================================
# NOTES
# =============================================================================
#
# Key differences from overfit experiments:
# - Dataset: Full training set (~560 groups) vs 32 groups
# - Duration: 10k epochs vs 20k epochs
# - Decoder: Unfreezes after 2500 steps vs stays frozen
# - Learning rate: 1e-4 (standard) vs 3e-4 (fast overfit)
# - Expected: Better generalization due to more data
#
# Staged training rationale:
# - Encoder needs time to learn task representations first
# - Joint training from start may overwhelm encoder with decoder gradients
# - Staged approach gives encoder a "head start"
#
# Timeline: ~12-18 hours per experiment on 4 GPUs
# Total: ~48-72 hours for all 4 architectures
#
