# References

<!--
CITATION GUIDE:
Use [@lpn] format in text, e.g., "as shown by [@trm]" or "[@chollet2019]"
These will be replaced with proper numbered citations during final compilation.
-->

## Core Papers

**[@chollet2019]** Chollet, F. (2019). On the Measure of Intelligence. *arXiv:1911.01547*.
https://arxiv.org/abs/1911.01547

**[@trm]** Jolicoeur-Martineau, A. (2025). Less is More: Recursive Reasoning with Tiny Networks. *arXiv:2510.04871*. 1st Place Paper Award, ARC Prize 2025.
https://arxiv.org/abs/2510.04871

**[@hrm]** Wang, G., Li, J., Sun, Y., Chen, X., Liu, C., Wu, Y., Lu, M., Song, S., & Abbasi Yadkori, Y. (2025). Hierarchical Reasoning Model. *arXiv:2506.21734*.
https://arxiv.org/abs/2506.21734

**[@lpn]** Bonnet, C., & Macfarlane, M. V. (2024). Searching Latent Program Spaces. *arXiv:2411.08706*. 3rd Place Paper Award, ARC Prize 2024.
https://arxiv.org/abs/2411.08706

## Analysis Papers

**[@arc-prize-2024]** Chollet, F., Knoop, M., Kamradt, G., & Landers, B. (2024). ARC Prize 2024: Technical Report. *arXiv:2412.04604*.
https://arxiv.org/abs/2412.04604

**[@arc-prize-2025]** Knoop, M. (2025). ARC Prize 2025: Results and Analysis. *ARC Prize Foundation Blog*.
https://arcprize.org/blog/arc-prize-2025-results-analysis

**[@hrm-analysis]** ARC Prize Foundation. (2025). The Hidden Drivers of HRM's Performance on ARC-AGI. *ARC Prize Foundation Blog*.
https://arcprize.org/blog/hrm-analysis

**[@trm-ttt]** McGovern, R. (2025). Test-time Adaptation of Tiny Recursive Models. *arXiv:2511.02886*.
https://arxiv.org/abs/2511.02886

**[@trm-inductive]** (2025). Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute. *arXiv:2512.11847*.
https://arxiv.org/abs/2512.11847

## Foundational Methods

**[@transformer]** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention Is All You Need. *Advances in Neural Information Processing Systems 30 (NeurIPS 2017)*.
https://arxiv.org/abs/1706.03762

**[@vae]** Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. *International Conference on Learning Representations (ICLR 2014)*.
https://arxiv.org/abs/1312.6114

**[@set-transformer]** Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., & Teh, Y. W. (2019). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. *International Conference on Machine Learning (ICML 2019)*.
https://arxiv.org/abs/1810.00825

**[@act]** Graves, A. (2016). Adaptive Computation Time for Recurrent Neural Networks. *arXiv:1603.08983*.
https://arxiv.org/abs/1603.08983

## Taxonomy & Framing

**[@neurosymbolic]** Chaudhuri, S., Ellis, K., Polozov, O., Singh, R., Solar-Lezama, A., & Yue, Y. (2021). Neurosymbolic Programming. *Foundations and Trends in Programming Languages, 7*(1-2), 1-151.
https://www.nowpublishers.com/article/Details/PGL-031

**[@induction-transduction]** Li, Y., et al. (2024). Combining Induction and Transduction for Abstract Reasoning. *arXiv:2411.02272*. 1st Place Paper Award, ARC Prize 2024.
https://arxiv.org/abs/2411.02272

**[@hemens-taxonomy]** Hemens, L. (2025). How to Beat ARC-AGI-2. *lewish.io*.
https://lewish.io/posts/how-to-beat-arc-agi-2

## Competition Solutions

**[@nvarc]** Sorokin, D., & Puget, J.-F. (2025). NVARC: ARC Prize 2025 1st Place Solution. NVIDIA.
https://www.kaggle.com/competitions/arc-prize-2025/discussion

**[@architects]** Fiedler, S., et al. (2024). the ARChitects: ARC Prize 2024 Kaggle Winner.
https://github.com/the-architects

---

## Code Availability

Our implementation is available at: https://github.com/bdsaglam/TinyRecursiveModels

This is a fork of the original TRM repository with encoder-based modifications for ETRM.

---

*Note: Citation numbers will be assigned during final compilation based on order of appearance in the text.*
