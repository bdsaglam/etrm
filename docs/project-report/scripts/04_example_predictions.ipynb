{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictions Figure\n",
    "\n",
    "**STATUS: DEFERRED** - Waiting for evaluations to complete.\n",
    "\n",
    "This notebook will generate a figure comparing example predictions across architectures:\n",
    "- Side-by-side comparison of model outputs\n",
    "- Ground truth vs predicted grids\n",
    "- Highlighting differences between encoder types\n",
    "\n",
    "Output: `docs/project-report/figures/example_predictions.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from figure_utils import (\n",
    "    setup_paper_style,\n",
    "    save_figure,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "# Apply paper styling\n",
    "setup_paper_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implementation\n",
    "\n",
    "When evaluations complete, implement the following:\n",
    "\n",
    "1. **Load evaluation results** from each final experiment (F1-F4)\n",
    "2. **Select representative puzzles** showing:\n",
    "   - Cases where models succeed\n",
    "   - Cases where models fail differently\n",
    "   - Cases highlighting encoder architecture differences\n",
    "3. **Create visualization** showing:\n",
    "   - Input grid\n",
    "   - Ground truth output\n",
    "   - Each model's prediction\n",
    "   - Difference highlighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARC color palette (official)\n",
    "ARC_COLORS = {\n",
    "    0: '#000000',  # Black\n",
    "    1: '#0074D9',  # Blue\n",
    "    2: '#FF4136',  # Red\n",
    "    3: '#2ECC40',  # Green\n",
    "    4: '#FFDC00',  # Yellow\n",
    "    5: '#AAAAAA',  # Gray\n",
    "    6: '#F012BE',  # Fuchsia\n",
    "    7: '#FF851B',  # Orange\n",
    "    8: '#7FDBFF',  # Teal\n",
    "    9: '#870C25',  # Brown\n",
    "}\n",
    "\n",
    "def plot_grid(ax, grid, title=None):\n",
    "    \"\"\"Plot a single ARC grid.\"\"\"\n",
    "    grid = np.array(grid)\n",
    "    h, w = grid.shape\n",
    "    \n",
    "    # Create colored image\n",
    "    img = np.zeros((h, w, 3))\n",
    "    for val, hex_color in ARC_COLORS.items():\n",
    "        r = int(hex_color[1:3], 16) / 255\n",
    "        g = int(hex_color[3:5], 16) / 255\n",
    "        b = int(hex_color[5:7], 16) / 255\n",
    "        mask = grid == val\n",
    "        img[mask] = [r, g, b]\n",
    "    \n",
    "    ax.imshow(img, interpolation='nearest')\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.set_xticks(np.arange(-0.5, w, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, h, 1), minor=True)\n",
    "    ax.grid(which='minor', color='white', linewidth=0.5)\n",
    "    ax.tick_params(which='both', size=0)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=8)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Example figure structure\n",
    "# This shows the intended layout\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "# Row labels\n",
    "row_labels = ['Puzzle 1', 'Puzzle 2']\n",
    "col_labels = ['Input', 'Ground Truth', 'F1: Standard', 'F2: Hybrid', 'F3: ETRMTRM']\n",
    "\n",
    "# Create placeholder grids\n",
    "for i, row_ax in enumerate(axes):\n",
    "    for j, ax in enumerate(row_ax):\n",
    "        # Placeholder grid\n",
    "        grid = np.random.randint(0, 10, (5, 5))\n",
    "        plot_grid(ax, grid)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_title(col_labels[j], fontsize=8)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(row_labels[i], fontsize=8)\n",
    "\n",
    "plt.suptitle('Example Predictions (PLACEHOLDER)', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️  This is a placeholder figure with random data.\")\n",
    "print(\"    Re-run this notebook after evaluations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading (TODO)\n",
    "\n",
    "After evaluations complete, load predictions from:\n",
    "- `outputs/etrm-final/F1_standard/predictions/`\n",
    "- `outputs/etrm-final/F2_hybrid_var/predictions/`\n",
    "- `outputs/etrm-final/F3_etrmtrm/predictions/`\n",
    "- `outputs/etrm-final/F4_lpn_var/predictions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load actual predictions when available\n",
    "# \n",
    "# def load_predictions(experiment_name):\n",
    "#     \"\"\"Load predictions from evaluation output.\"\"\"\n",
    "#     pred_dir = Path(f\"../../outputs/etrm-final/{experiment_name}/predictions\")\n",
    "#     if not pred_dir.exists():\n",
    "#         print(f\"Predictions not found: {pred_dir}\")\n",
    "#         return None\n",
    "#     \n",
    "#     predictions = {}\n",
    "#     for pred_file in pred_dir.glob(\"*.json\"):\n",
    "#         puzzle_id = pred_file.stem\n",
    "#         with open(pred_file) as f:\n",
    "#             predictions[puzzle_id] = json.load(f)\n",
    "#     \n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATUS: WAITING FOR EVALUATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis notebook is a placeholder.\")\n",
    "print(\"Once evaluations complete for F1-F4, re-run to generate:\")\n",
    "print(\"  - docs/project-report/figures/example_predictions.png\")\n",
    "print(\"\\nThe figure will compare predictions across architectures\")\n",
    "print(\"to highlight differences in model behavior.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
