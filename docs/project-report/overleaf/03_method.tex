% 03_method.tex
% LaTeX version of Method section for Overleaf.
% Requires bibliography keys: trm, transformer, set-transformer, vae, act, lpn

\section{Method}
\label{sec:method}

\subsection{Overview: From memorization to generalization}

The Tiny Recursive Model (TRM) \cite{trm} achieves strong performance on ARC-AGI through recursive reasoning with deep supervision. However, TRM relies on a learned embedding matrix that maps puzzle identifiers to task-specific representations. Critically, this embedding matrix includes entries for \emph{both} training and evaluation puzzles—their embeddings receive gradient updates during training even though their test queries are held out. This design choice means TRM cannot solve puzzles without corresponding embeddings, fundamentally limiting it to interpolation rather than true generalization.

We propose \textbf{Encoder-based TRM (ETRM)}, which replaces the embedding lookup with a neural encoder that computes task representations from demonstration input--output pairs at inference time. This modification transforms TRM from a memorization-based system into one capable of few-shot learning:

\begin{align}
\text{TRM:}\quad & \mathbf{c} = \text{EmbeddingMatrix}[\text{puzzle\_id}] \\
\text{ETRM:}\quad & \mathbf{c} = \text{Encoder}\big(\{(\mathbf{x}_i^{\text{in}},\mathbf{x}_i^{\text{out}})\}_{i=1}^K\big)
\end{align}

where $\mathbf{c}\in\mathbb{R}^{T\times D}$ is the task context ($T=16$ tokens, $D=512$ dimensions), and $K$ is the number of demonstration pairs. The encoder learns to extract transformation rules from demonstrations, enabling generalization to puzzles not seen during training. We preserve TRM's decoder architecture unchanged \cite{trm}; only the task-context source is modified.

\subsection{Background: TRM architecture}

\subsubsection{Task context via puzzle embedding lookup}

TRM obtains task context through a learned embedding matrix indexed by puzzle identifier. Pseudocode:

\begin{verbatim}
# TRM: Task context from embedding lookup
def trm_get_context(puzzle_id):
    # puzzle_emb: learned matrix of shape (num_puzzles, T, D)
    # Includes entries for ALL puzzles (training AND evaluation)
    return puzzle_emb[puzzle_id]  # Returns (T, D) context
\end{verbatim}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.75\linewidth]{trm.png}
  \caption{TRM architecture: task context is obtained via embedding matrix lookup indexed by puzzle ID.}
  \label{fig:trm-arch}
\end{figure}

The embedding matrix must include an entry for every puzzle the model will encounter. Since evaluation puzzle IDs receive gradient updates during training, TRM cannot generalize to truly unseen puzzles—it effectively memorizes task-specific mappings.

\subsubsection{Dual-state recursive reasoning}

Given task context, TRM maintains two latent states that are iteratively refined:
\begin{itemize}
  \item $\mathbf{y}$: current predicted solution (embedded)
  \item $\mathbf{z}$: latent reasoning state
\end{itemize}

Pseudocode for the forward pass:

\begin{verbatim}
def trm_forward(x, puzzle_id, y, z, H_cycles=3, L_cycles=6):
    # Get task context via embedding lookup (not from demos!)
    c = puzzle_emb[puzzle_id]         # <-- The limitation

    input_emb = embed(x) + c          # Add task context to input

    for h in range(H_cycles):
        for l in range(L_cycles):
            z = f_theta(z, y + input_emb)  # Refine reasoning
        y = f_theta(y, z)                   # Update solution

    return y, z
\end{verbatim}

The network $f_\theta$ is a small 2-layer transformer \cite{transformer}. Recursive application creates an effective depth of $H\times(L+1)\times 2 = 42$ layers.

\subsubsection{Deep supervision with adaptive computation}

TRM uses deep supervision: the carry state $(\mathbf{y},\mathbf{z})$ persists across training steps, with supervision applied at each recursion. A Q-head implements Adaptive Computation Time (ACT) \cite{act} to learn halting; during training an exploration probability $p_{\text{explore}}$ encourages continued computation sometimes.

\subsection{Encoder architectures}

We study three encoder paradigms, each reflecting a different hypothesis about effective task representation.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{etrm.png}
  \caption{ETRM architecture: task context is computed from demonstration pairs via a neural encoder, replacing the embedding lookup.}
  \label{fig:etrm-arch}
\end{figure}

\subsubsection{Feedforward deterministic encoder}

Hypothesis: a single forward pass through a feedforward encoder can extract sufficient information from demos to characterize the transformation rule.

The encoder has two stages:

Stage 1 -- Per-demo grid encoding. Each demo pair $(\mathbf{x}_i^{\text{in}},\mathbf{x}_i^{\text{out}})$ is encoded independently by concatenating input and output grids and processing with a transformer:

\[
\mathbf{h}_i = \text{TransformerEnc}\big([\mathbf{x}_i^{\text{in}};\mathbf{x}_i^{\text{out}}]\big)\in\mathbb{R}^{2S\times D}
\]

with $S=900$ (flattened 30$\times$30 grid). Pool to a single vector:

\[
\mathbf{e}_i = \text{MeanPool}(\mathbf{h}_i \odot \mathbf{m}_i)\in\mathbb{R}^D
\]

where $\mathbf{m}_i$ masks padding tokens.

Stage 2 -- Set aggregation via cross-attention. Aggregate the $K$ demo encodings $\{\mathbf{e}_i\}_{i=1}^K$ using learnable query tokens \cite{set-transformer}:

\[
\mathbf{c} = \text{CrossAttn}(\mathbf{Q}, \mathbf{E}, \mathbf{E}) \in \mathbb{R}^{T\times D}
\]

where $\mathbf{Q}\in\mathbb{R}^{T\times D}$ are $T=16$ learnable queries and $\mathbf{E}\in\mathbb{R}^{K\times D}$ stacks demo encodings.

Pseudocode:

\begin{verbatim}
def deterministic_encoder(demo_inputs, demo_outputs, demo_mask):
    # Stage 1: Encode each demo pair
    B, K, S = demo_inputs.shape
    encodings = []
    for i in range(K):
        h = transformer_enc(concat(demo_inputs[:,i], demo_outputs[:,i]))
        e = mean_pool(h, mask=demo_mask[:,i])
        encodings.append(e)
    E = stack(encodings, dim=1)  # (B, K, D)

    # Stage 2: Cross-attention aggregation
    Q = learnable_queries.expand(B, -1, -1)  # (B, T, D)
    context = cross_attention(Q, E, E, mask=demo_mask)
    return context  # (B, T, D)
\end{verbatim}

\subsubsection{Feedforward variational encoder}

Hypothesis: A variational bottleneck promotes a structured latent space of transformation rules that may improve generalization.

We study two variants.

\paragraph{Cross-attention VAE.} Apply variational bottleneck \emph{after} aggregating demos via cross-attention. This allows the model to capture cross-demo patterns before compressing to a latent code.

\begin{verbatim}
def crossattn_vae_encoder(demo_inputs, demo_outputs, demo_mask):
    # Stage 1: Deep per-demo encoding (8 layers, CLS pooling)
    encodings = []
    for i in range(K):
        h = deep_transformer(concat(demo_inputs[:,i], demo_outputs[:,i]))
        e = cls_pooling(h)  # (B, D)
        encodings.append(e)
    E = stack(encodings, dim=1)  # (B, K, D)

    # Stage 2: Cross-attention aggregation
    Q = learnable_queries.expand(B, -1, -1)  # (B, T, D)
    context = cross_attention(Q, E, E, mask=demo_mask)  # (B, T, D)

    # Stage 3: Variational bottleneck
    z_pre = mean_pool(context)  # (B, D)
    z_pre = rms_norm(z_pre)     # Stabilize before projection
    mu, logvar = linear_proj(z_pre), linear_proj(z_pre)
    logvar = clamp(logvar, -10, 10)

    # Reparameterization (training only)
    z = mu + exp(0.5 * logvar) * randn_like(mu) if training else mu

    # Decode to output shape
    context = reshape(linear(z), (B, T, D))
    return context
\end{verbatim}

\paragraph{Per-demo VAE.} Inspired by LPN \cite{lpn}, apply variational inference \emph{per demo} before aggregation. Each demo gets its own latent code, which are then averaged.

\begin{verbatim}
def perdemo_vae_encoder(demo_inputs, demo_outputs, demo_mask):
    # Stage 1: Shallow per-demo encoding (2 layers, 128 hidden)
    encodings = []
    for i in range(K):
        h = shallow_transformer(concat(demo_inputs[:,i], demo_outputs[:,i]))
        e = cls_pooling(h)  # (B, D_internal=128)
        encodings.append(e)
    E = stack(encodings, dim=1)  # (B, K, 128)

    # Stage 2: Per-demo variational projection
    mu = linear_proj(E)      # (B, K, latent_dim=32)
    logvar = linear_proj(E)  # (B, K, 32)
    logvar = clamp(logvar, -10, 10)

    # Reparameterize each demo independently
    z_demos = mu + exp(0.5 * logvar) * randn_like(mu) if training else mu

    # Stage 3: Mean aggregation (after sampling)
    z_demos = z_demos * demo_mask.unsqueeze(-1)  # Mask invalid
    z_pooled = sum(z_demos, dim=1) / num_valid_demos  # (B, 32)

    # Project to output shape
    context = reshape(linear(z_pooled), (B, T, D))
    return context
\end{verbatim}

Cross-attention VAE captures cross-demo patterns before bottlenecking; per-demo VAE is more compact ($\sim$1M params vs.\ $\sim$16M) but may lose demo interactions. We adopt LPN's encoder design for comparison but \emph{do not} use test-time gradient search; our encoders produce fixed representations.

\subsubsection{Iterative encoder (joint encoder--decoder refinement)}

Hypothesis: Recursive refinement that benefits the decoder might also benefit the encoder. The iterative encoder mirrors TRM's decoder structure.

Encoder maintains dual latent states:
\begin{itemize}
  \item $\mathbf{z}_e^H$: high-level context (serves as decoder input)
  \item $\mathbf{z}_e^L$: low-level reasoning state
\end{itemize}

Hierarchical refinement (ACT-style) pseudocode:

\begin{verbatim}
def iterative_encoder_step(z_e_H, z_e_L, demo_input, H_cycles, L_cycles):
    # demo_input: aggregated demo representation

    for h in range(H_cycles):
        for l in range(L_cycles):
            z_e_L = L_level(z_e_L, z_e_H + demo_input)
        z_e_H = L_level(z_e_H, z_e_L)

    return z_e_H, z_e_L  # z_e_H is context for decoder
\end{verbatim}

Unlike feedforward encoders, the iterative encoder's carry state persists across ACT steps. The encoder refines its understanding as the decoder refines its prediction—a form of joint reasoning.

\subsection{Integration with TRM decoder}

For all encoder types, integration follows:
\begin{enumerate}
  \item \textbf{Context injection:} encoder output $\mathbf{c}\in\mathbb{R}^{T\times D}$ replaces puzzle embedding.
  \item \textbf{Position concatenation:} context is prepended to input token embeddings.
  \item \textbf{Forward pass:} combined representation flows through TRM's recursive reasoning.
\end{enumerate}

Decoder architecture remains as in TRM \cite{trm}: dual-state design ($\mathbf{y},\mathbf{z}$), H/L cycles, ACT halting, and deep supervision.

\subsection{Training protocol}

\subsubsection{Strict train/evaluation separation}

We enforce full data separation:

\begin{tabular}{l l l}
\hline
Split & Puzzles & Purpose \\
\hline
Training & $\sim$560 groups (ARC-AGI training + concept) & Model training \\
Evaluation & $\sim$400 groups (ARC-AGI evaluation) & Testing generalization \\
\hline
\end{tabular}

Evaluation puzzles' demonstrations are never seen during training; the encoder must extract rules at test time.

\subsubsection{Data augmentation}

Following TRM, we apply $\approx$1000 augmented versions per puzzle:
\begin{itemize}
  \item Color permutation: random shuffle of colors 1--9 (black=0 fixed)
  \item Dihedral transforms: 8 geometric transforms (4 rotations $\times$ 2 reflections)
  \item Translation: random positioning within 30$\times$30 grid (training only)
\end{itemize}

The same augmentation is applied to all components of a puzzle (demos and tests) to preserve the transformation structure.

\subsubsection{Pretrained decoder initialization}

We initialize the decoder from TRM's pretrained weights, providing:
\begin{enumerate}
  \item Faster convergence: decoder already knows recursive refinement for ARC tasks.
  \item Encoder focus: encoder can concentrate on learning representations.
\end{enumerate}

We explore two modes:
\begin{itemize}
  \item Frozen decoder: only encoder updated (faster, ensures encoder learns).
  \item Joint fine-tuning: both encoder and decoder updated (potentially better final performance).
\end{itemize}

\subsubsection{Variational training}

For VAE encoders, the training loss is:
\[
\mathcal{L} = \mathcal{L}_{\text{CE}} + \beta\cdot\mathcal{L}_{\text{KL}}
\]

Practical considerations:
\begin{itemize}
  \item KL weight: we use a fixed $\beta=0.0001$.
  \item Numerical stability: clamp $\log\sigma^2\in[-10,10]$.
  \item Evaluation: use mean $\boldsymbol{\mu}$ without sampling for deterministic evaluation.
\end{itemize}

\subsection{Design space summary}

\begin{table}[ht]
  \centering
  \caption{Encoder architecture design space}
  \begin{tabular}{p{0.28\linewidth} p{0.2\linewidth} p{0.18\linewidth} p{0.18\linewidth} p{0.18\linewidth}}
    \hline
    Encoder type & Aggregation & Variational & Parameters & Key property \\
    \hline
    Feedforward deterministic & Cross-attention & No & $\sim$15M & Simple, stable baseline \\
    Cross-attention VAE & Cross-attention & Post-aggregation & $\sim$16M & Regularized latent space \\
    Per-demo VAE & Mean & Per-demo & $\sim$1M & LPN-inspired, compact \\
    Iterative (TRM-style) & Mean & No & $\sim$8M & Joint refinement with decoder \\
    \hline
  \end{tabular}
\end{table}

Each architecture embodies hypotheses: cross-attention vs.\ mean pooling, variational vs.\ deterministic, iterative vs.\ feedforward. We evaluate these empirically in Section~\ref{sec:results}.

