% Section 5: Discussion
\section{Discussion}
\label{sec:discussion}

\subsection{Analysis of Results}

\subsubsection{The Generalization Gap}

The training--test disconnect observed in Section~4.4 admits two hypotheses:

\paragraph{(A) Task Difficulty.} Extracting transformation rules from demonstrations in a single forward pass—without any feedback signal—is fundamentally harder than refining puzzle-specific embeddings over many gradient updates. The encoder must perform meta-learning: learning to learn from examples \citep{trm}.

\paragraph{(B) Implementation Issues.} Our encoder designs, training procedures, or hyperparameters may have flaws.

Evidence favors hypothesis (A). Three fundamentally different architectures all failed identically. The ETRM-Iterative experiment is particularly informative: iteration analogous to TRM's recursive decoder still yielded 0\% test accuracy, suggesting the problem is not insufficient computation but rather the absence of a guiding signal during refinement.

We cannot fully rule out (B), but the consistent failure pattern points toward task difficulty.

\subsubsection{The Asymmetry Between TRM and ETRM}

Understanding why TRM succeeds while ETRM fails requires examining how each learns transformation rules.

\paragraph{How TRM learns.} TRM \citep{trm} refines each puzzle embedding over hundreds of thousands of training steps. Each of the $\sim$876,000 puzzle-augmentation combinations receives approximately 500+ gradient updates during training. The embedding gradually captures task-specific patterns through this extended optimization—a significant advantage where the model has many opportunities to learn each transformation.

\paragraph{What we ask the encoder to do.} ETRM's encoder must extract the transformation rule from just 2--5 demonstration pairs in a single forward pass, producing a representation that works for transformations never seen during training. This is meta-learning: the encoder must learn \emph{how to learn} from examples.

The difficulty gap becomes clearer when we compare refinement mechanisms:

\begin{table}[h]
\centering
\caption{Refinement mechanisms comparison}
\label{table:refinement}
\begin{tabular}{p{0.28\linewidth} p{0.36\linewidth} p{0.30\linewidth}}
\hline
Approach & Refinement & Feedback Signal \\
\hline
TRM \citep{trm} & Gradient descent on embedding & Ground-truth labels (supervised) \\
LPN \citep{lpn} & Gradient ascent in latent space & Demo consistency (self-supervised) \\
ETRM (feedforward) & Single forward pass & None \\
ETRM-Iterative & Multiple encoder iterations & None \\
\hline
\end{tabular}
\end{table}

Our ETRM-Iterative experiment tested whether iteration alone could help—it could not. The issue is not computation but feedback. TRM's refinement is guided by gradients from ground-truth labels. LPN's test-time search is guided by leave-one-out demo consistency. ETRM's encoder iterates without any signal indicating whether its representation is improving.

\textbf{Takeaway:} Effective latent-space refinement requires a feedback signal—either supervised (labels) or self-supervised (demo consistency). Unguided iteration is insufficient.

\subsubsection{Implications of Encoder Collapse}

The encoder collapse documented in Section~4.4.2 explains the training--test disconnect. With near-constant encoder outputs, the decoder receives essentially the same ``task representation'' for every puzzle, so it learns to ignore the uninformative encoder signal and instead memorizes input--output mappings for training examples directly \citep{hrm-analysis}. High training accuracy through memorization; zero generalization to new transformations.

\subsubsection{Did Variational Encoding Help?}

We hypothesized that variational encoders might encourage more diverse representations through KL regularization. The Cross-Attention VAE achieved an order-of-magnitude higher cross-sample variance (3.33 vs.\ 0.36) compared to the deterministic encoder—yet still 0\% test accuracy.

Higher variance does not equal useful variance. The variational encoder produces diverse representations, but these representations are not discriminative for transformation rules. The KL penalty toward a standard normal prior may push representations away from task-relevant structure. Additionally, VAE regularization prevented decoder memorization (lower 40.62\% training accuracy) without producing transformation-relevant features.

\textbf{Takeaway:} Variance alone is insufficient; representations must capture transformation-relevant information. Diversity without discriminability does not enable generalization.

\subsubsection{Qualitative Failure Modes}

Examination of ETRM predictions on held-out puzzles (Figure~6, Section~4.4.3) reveals several failure modes:
\begin{enumerate}
  \item \textbf{Collapsed outputs.} Some predictions are nearly uniform (solid color), directly reflecting encoder collapse propagating to the decoder.
  \item \textbf{Structured but wrong.} Some predictions show grid structure and color patterns, but the wrong transformation is applied—e.g., color filling where rotation was required.
  \item \textbf{Partial correctness.} Occasionally, predictions capture some aspect of the transformation (correct colors, wrong arrangement), suggesting the decoder has learned general grid manipulation skills.
\end{enumerate}

These patterns are consistent with a model that has learned \emph{something} about ARC transformations from training but cannot select the correct transformation without a useful encoder signal. In contrast, TRM predictions (with puzzle embeddings) are often correct or close, demonstrating the decoder is capable when given appropriate task context.

\subsubsection{Context for Comparison with TRM}

Direct comparison between TRM and ETRM requires acknowledging a fundamental difference in what each model is asked to do:

\begin{table}[h]
\centering
\caption{Context for TRM/ETRM comparison}
\label{table:context}
\begin{tabular}{l c c l}
\hline
Model & Pass@1 & Train Acc & Task \\
\hline
TRM \citep{trm} (155k steps) & 37.38\% & 92.50\% & Generalize to augmented versions of known puzzles \\
TRM \citep{trm} (518k steps) & 41.75\% & 98.44\% & Same \\
ETRM-Deterministic (175k steps) & 0.00\% & 78.91\% & Generalize to entirely unseen transformations \\
\hline
\end{tabular}
\end{table}

TRM test puzzles have embeddings that receive gradient updates during training—it generalizes to different augmentations (color permutations, rotations) of puzzles it has ``seen.'' ETRM must generalize to transformations it has never encountered, a fundamentally harder task.

Our encoder approach does not achieve few-shot generalization. However, the comparison is not entirely fair. A more appropriate comparison would be with LPN \citep{lpn}, which achieves 15.5\% on held-out puzzles—but only with test-time gradient optimization that we deliberately avoided.

\subsection{Challenges Encountered}

Several practical challenges emerged during development that required careful debugging.

\subsubsection{Gradient Starvation}

Our initial implementation cached encoder outputs across ACT steps for efficiency. This turned out to be a significant mistake. With TRM's dynamic halting mechanism, only samples that reset their carry state receive fresh encoder outputs—roughly 2\% of the batch per step. The encoder therefore received gradients from only 2\% of training data, severely stunting its learning. Training accuracy plateaued around 35--50\% with the encoder producing essentially random outputs.

The fix was straightforward once diagnosed: re-encode the full batch at every ACT step rather than caching. This ensures 100\% gradient coverage at the cost of additional computation. The lesson here is that gradient flow analysis is critical when modifying architectures with dynamic computation patterns—efficiency optimizations that seem harmless can silently break learning.

\subsubsection{Training Stability}

Early experiments exhibited sudden training collapse around step 1900. The encoder output distribution was shifting faster than the decoder could adapt, creating a feedback loop that destabilized both components. Gradient clipping (\texttt{grad\_clip\_norm}=1.0) resolved this issue and stabilized training throughout all subsequent experiments.

\subsubsection{Monitoring Representation Quality}

Standard training metrics (loss, accuracy) failed to reveal encoder collapse until evaluation. We introduced cross-sample variance as a diagnostic metric, tracking how differently the encoder responds to different puzzles within each batch. Low variance proved to be an early warning sign of representation collapse, often visible long before evaluation revealed the problem. This metric proved essential for debugging and would be valuable for future encoder-based approaches.

\subsection{Limitations}

\subsubsection{Computational Constraints}

Our experiments operated under significant resource constraints:
\begin{itemize}
  \item \textbf{Training duration:} 25k--175k steps vs.\ TRM's 518k steps (\(\sim\)4 days per run on 4 GPUs).
  \item \textbf{Evaluation scope:} 32 puzzle groups (8\%) instead of full 400 (\(\sim\)1 day on 4 GPUs for full evaluation).
  \item \textbf{Single seed:} No variance estimates across runs.
\end{itemize}

These results provide directional signal—0\% vs.\ 37\% accuracy is unambiguous—but absolute performance numbers might improve with additional training.

\subsubsection{Architecture Exploration}

We tested three encoder paradigms but did not explore alternatives that might succeed:
\begin{itemize}
  \item Slot attention for object-centric encoding.
  \item Graph neural networks for relational reasoning.
  \item Contrastive objectives for discriminative representations.
\end{itemize}

A different architecture might succeed where ours failed.

\subsubsection{Implementation Caveats}

We cannot fully rule out implementation issues:
\begin{itemize}
  \item Encoder architectures may be suboptimal.
  \item Training dynamics may have undetected problems.
  \item Hyperparameters may be poorly tuned.
\end{itemize}

One known issue: EMA weights from the pretrained TRM checkpoint were not properly loaded for the decoder. This is unlikely to explain the generalization failure because the decoder was trainable and reached 79\% training accuracy—the problem is generalization, not learning capacity.

The consistent failure across three architectures suggests task difficulty rather than implementation bugs, but more exploration is warranted.

\subsection{Future Directions}

Our results suggest that computing task representations in a single forward pass is insufficient for extracting transformation rules. Drawing on the program-synthesis taxonomy from Section~2, we identify promising directions.

\subsubsection{Self-Supervised Test-Time Search}

Our ETRM-Iterative experiment showed that iteration alone is insufficient—the encoder refines its representation but has no signal indicating whether it is improving. Both TRM \citep{trm} and LPN \citep{lpn} succeed because they have feedback signals guiding refinement: TRM uses label gradients during training, LPN uses demo consistency at test time.

LPN demonstrates that self-supervised gradient search in latent space can significantly improve generalization (7.75\% to 15.5\%). Combining ETRM with test-time optimization could provide the missing ingredient:
\begin{itemize}
  \item \textbf{Leave-one-out loss:} Use held-out demo pairs as self-supervision for latent-space search—no labels required, only the demos themselves.
  \item \textbf{Hybrid search:} Encoder provides a warm start, gradient-based refinement provides the feedback loop.
  \item \textbf{Efficiency:} Starting from a learned encoder estimate may require fewer gradient steps than LPN's random initialization.
\end{itemize}

The common thread in successful approaches is \emph{guided refinement}. Our failed ETRM-Iterative suggests unguided iteration is not enough—but guided iteration at test time may bridge the gap.

\subsubsection{Contrastive Learning for Encoder}

An alternative to test-time optimization is training a more discriminative encoder:
\begin{itemize}
  \item Train encoder to produce similar representations for demos from the same puzzle, and different representations for demos from different puzzles.
  \item This could encourage discriminative representations without requiring test-time optimization.
\end{itemize}

\subsubsection{Improved Decoder Initialization}

Our decoder initialization did not include EMA weights from the pretrained TRM checkpoint. While unlikely to explain the generalization failure—the decoder reached 79\% training accuracy—future experiments should use properly EMA-initialized weights to eliminate this confound.

\subsection{Conclusions}

Our experiments reveal that replacing TRM puzzle embeddings with a demonstration encoder is substantially harder than expected. Three encoder architectures—feedforward deterministic, variational, and iterative—all achieve reasonable training accuracy but complete failure on held-out puzzles. Analysis shows this stems from encoder collapse: the encoders produce near-constant outputs regardless of input demonstrations, forcing the decoder to memorize training patterns rather than learn generalizable rule extraction.

The key insight is the asymmetry between TRM and ETRM. TRM refines each puzzle embedding over hundreds of gradient updates during training. ETRM asks the encoder to extract equivalent information in a single forward pass with no task-specific feedback. Our ETRM-Iterative experiment shows that iteration alone does not solve this problem—the missing ingredient is a feedback signal guiding refinement.

The most promising path forward is combining ETRM's encoder with test-time optimization using self-supervised signals, as demonstrated by LPN \citep{lpn}. This would provide the guided refinement that successful approaches share while preserving the ability to generalize to truly novel puzzles.

