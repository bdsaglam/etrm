<!-- image -->

## Literature Review: Approaches to the ARC-AGI Benchmark

## Introduction and Background

The Abstraction and Reasoning Corpus (ARC) , introduced by François Chollet in 2019, is a benchmark designed to measure fluid intelligence in AI - the ability to rapidly learn new concepts and solve novel problems from only a few examples . ARC-AGI (as it was later renamed) consists of 2D grid-based puzzles (see Fig. 1) where each task provides a handful of input-output demonstrations and requires producing outputs for new inputs by inferring the underlying rule . Crucially, every ARC task is unique and cannot be anticipated during training ,   so   the benchmark tests broad generalization rather than pattern recall . Humans solve nearly all ARC tasks with ease, leveraging core knowledge like objectness and spatial   relations,   whereas   early   AI   systems   struggled   mightily .   In   fact,   direct   deep   learning approaches initially performed near-zero :   a   2020   evaluation found no neural network model above 1% accuracy   (GPT-3   scored   0%) .   Five   years   on, ARC-AGI   remains   unsolved ,   though   progress   has accelerated - rising from ~20% in 2019 to over 55% by 2024 - thanks to new hybrid approaches . ARC is now considered 'the most important unsolved AI benchmark' for general reasoning . 1 2 3 4 5 2 6 6

A key dichotomy in ARC research has been induction vs. transduction . Inductive solutions aim to infer an explicit program or rule from the examples, then apply it to the test input . Transductive solutions instead try to directly predict the outputs from inputs and examples, for instance via a neural network, without deriving an interpretable rule . Historically, most successful ARC solvers used induction (searching for a program that transforms input grids to output grids), as purely neural methods failed to generalize to the unseen tasks. Only recently have transductive approaches begun to make headway, often by   incorporating   some   form   of test-time   learning or   adaptation.   Today,   the state-of-the-art   uses   a combination of both paradigms: as Chollet et al. note, the best standalone induction-only or transductiononly models plateau around 40% accuracy, while ensembles mixing the two have pushed past 50% . In the following, we review the major approach categories and key contributions from the last five years, situating each in the induction-transduction spectrum. 7 8 9 10 11

## Program Synthesis Approaches (Induction)

Brute-Force Program Search: The earliest and most straightforward ARC solvers treated each task as a program synthesis problem in a restricted domain-specific language. The winning entry of the first ARC competition   (Kaggle   2020,   user 'icecuber' )   hand-crafted   a   DSL   with   142   primitive   operations   and exhaustively searched for a sequence of operations that fit the given examples .   This   brute-force approach, augmented with clever pruning, could solve a significant fraction of the puzzles given enough compute. In fact, analysis showed that by ensembling many such search-based solvers, almost half of the 100 hidden test tasks could be solved through pure brute force . This was an existence proof that a universal (but horribly inefficient) program-search could cover many ARC tasks, consistent with the notion that the simplest program explaining the examples is likely correct (an application of Occam's Razor and 12 13

minimum description length) . However, brute-force search suffers from combinatorial explosion - the space of programs is enormous - so later work focused on making the search more efficient and guided . 14

Guided and Structured Search: Several projects introduced structural priors to reduce the search space. Xu et al. (2022) proposed representing the grid as a graph of objects and relationships, then performing constraint-guided search in a graph-based DSL .   By   converting   pixel   grids   into   higher-level   object graphs, their solver could prune infeasible programs early, though it was limited to tasks where input/ output grid sizes match . Ainooson et al. (2023) took inspiration from human cognition, developing a Visual Imagery Reasoning Language (VIMRL) that encodes core knowledge priors (like object permanence and symmetry) for multi-level program synthesis . This 'neurodiversity-inspired' approach allowed more flexible use of spatial primitives and reportedly handled certain tasks more gracefully than purely low-level DSLs . Overall, these works remained in the inductive paradigm - they synthesize explicit programs - but improved search efficiency via better abstractions. 15 15 16 16

Neuro-Symbolic Program Learning: Bridging learning and search, some approaches use neural networks to guide program synthesis. DreamCoder ,   a   neural program synthesis system, was adapted to ARC by Bober-Irizar and Banerjee (2024). They designed a custom DSL called PeARL (Perceptual Abstraction and Reasoning Language) tailored to ARC and trained a neural recognition model to propose high-level program components . This neurosymbolic solver could automatically invent new subroutines and significantly outperformed prior implementations on ARC . Banburski et al. (2020) had earlier advocated a similar modular approach: they implemented a set of core knowledge modules (e.g.   functions   for   counting objects, moving shapes, etc.) and used a neural controller to assemble these modules into solutions . In their 'Dreaming with ARC' report, they argue that a modular, neural-guided program synthesis system can adapt to each ARC task by activating the appropriate primitives . These methods still produce an explicit program (induction), but the search is informed by learned intuition - a strategy Chollet had predicted early on as a promising path . 17 17 18 18 19 20

Inductive Logic and Symbolic AI: Not all induction approaches rely on brute force or neural guidance; some   leverage   symbolic   learning   techniques.   Rocha et   al. (2024)   presented   an inductive   logic programming (ILP) approach to ARC . They defined an object-centric DSL with a small set of logical primitives (e.g. color , shape, position relations). Given a few examples, their ILP system searches for a logic program (in Prolog style) that covers the examples and can be applied to the test input . Because ILP can induce rules from very few examples, this method can generalize to unseen tasks without any gradient-based learning - essentially performing symbolic induction. In experiments on a subset of ARC tasks, the ILP solver could indeed find correct programs for tasks that fit its predefined primitives . While limited by the completeness of the chosen primitives, this demonstrates the value of classical AI techniques (symbolic reasoning, search, logic) in the ARC domain, in contrast to purely statistical learners. 21 21 22

Large Language Models for Program Synthesis: A major development around 2023-2024 was the use of pre-trained LLMs to   generate candidate programs. Greenblatt (2024) showed that GPT-4 (with coding capabilities) can be harnessed to write Python programs solving ARC tasks .   His approach prompted GPT-4 with the task's I/O examples and had it sample a large number of programs , which were then executed to check if any produced the correct output. By generating thousands of candidates (and even employing GPT-4   to   debug   and   refine   them   iteratively),   this   method   achieved   impressive   results   -   reportedly discovering solutions for many tasks and revealing a scaling law (more samples yield more solved tasks) . The downside is extreme compute cost , but it validated that LLMs can learn-to-code ARC solutions in an open-ended language (Python) given the right prompting. Relatedly, Romera-Paredes et   al. (2023) 23 24

proposed FunSearch , an evolutionary program search where an LLM repeatedly modifies programs based on a fitness score .   In   their   framework,   the   LLM   treats   two   current   programs   and   their   scores   as 'parents' and generates a new program that hopefully inherits the good traits. A critical challenge here is defining a gradual scoring function - ARC normally gives only a binary correct/incorrect signal. To address that, Singhal et al. (2024) introduced concept-based scoring : one variant used a CNN to judge how well a program's output matches the desired pattern, and another variant used an LLM to compare the program's output to the textual description of the task . By giving continuous feedback, they could guide the LLM search more effectively. In a test on 50 public tasks, this concept-guided FunSearch reportedly boosted the success rate from 26% (direct GPT-4 prompting) to 50-58% when the vision and language feedback were incorporated . This is a remarkable jump achieved by combining LLM-driven search (induction) with richer evaluation signals. It exemplifies how the line between induction and transduction can blur: the system uses a neural network (GPT-4 or CNN) during the search for a program, effectively learning on the fly which candidate programs are promising - a synergy of symbolic search with learned perception. 25 26 27 28

In summary, inductive approaches have dominated ARC-AGI for most of its history. They aim to find an interpretable solution program for each puzzle. Over the last five years, such approaches evolved from raw brute-force enumeration to hybrid neurosymbolic systems that leverage machine learning to make search feasible. Hand-crafted DSLs (for efficiency and as a bias towards the kind of operations humans use ) are a common theme . Whether via deep learning guidance , logical reasoning, or evolutionary search, these methods embrace the principle that the simplest program consistent with the examples is likely correct . Their strengths are precision and transparency - many can output human-readable solution scripts - but they can struggle with tasks that don't decompose cleanly into the provided primitives or that require a more holistic, perception-like insight. 29 30 30

## Direct Neural Network Approaches (Transduction)

Pure deep learning approaches historically fared poorly on ARC, since neural models trained in the conventional way lack the ability to handle entirely novel tasks outside their training distribution . The classic paradigm of training on many examples of each task is infeasible - by design, each ARC task is unique . Until 2023, virtually all neural end-to-end attempts yielded near-zero scores (random guessing level) . However, recent research has shown that neural networks can tackle ARC if they are allowed to adapt at test time or are designed with special architectures. Below we outline the key developments on the transductive side: 4 2

- Test-Time Training (TTT): One breakthrough was the realization that a model can update its parameters during inference using the few examples of the new task. This idea, known as test-time training or inference-time fine-tuning ,   treats   the   provided   I/O   pairs   as   a   tiny   training   set   to specialize the model for that specific puzzle. Akyürek et   al. (2024) demonstrated the surprising effectiveness of this strategy for ARC . They started with a large pre-trained language model (GPT-style) and fine-tuned it on a set of similar reasoning tasks (generated via prompts and data augmentation) to give it generic puzzle-solving ability. Then for each ARC task, they temporarily trained the model on the 2-3 demonstration pairs (using a self-supervised objective predicting withheld parts of the grid, etc.), before having it produce the answer for the test input. This test-time adaptation yielded dramatic improvements: their 8-billion-parameter model achieved 53% accuracy on ARC's public evaluation (compared to ~28% without TTT) .   This is a nearly 25% absolute jump over previous neural-only results, and notably, by ensembling the TTT model with a programsynthesis solver , they reached a new state-of-the-art on the public set comparable to an average · 31 32 32

human's score .   TTT   essentially   gives   the   network   a   chance   to learn the task on the fly , circumventing the static nature of conventional inference. It blurs the train-test boundary, enabling a form of transductive learning where the model optimizes for each test instance. The success of Akyürek et al.'s approach has made test-time adaptation a cornerstone: as an ARC review noted, 'some form of test time adaptation is absolutely crucial' for good performance on this benchmark . 32 33

- Hierarchical and Iterative Reasoning Models: Another path to improve neural generalization is to design architectures that simulate   human   problem-solving .   In   2025,   Wang et   al. introduced the Hierarchical Reasoning Model (HRM) ,   a   bespoke neural network inspired by the brain's multi-level processing. HRM is a recurrent model with two interleaved modules: a high-level module H that plans a solution in abstract terms, and a low-level module L that executes details . The model runs for a few iterations (a controlled 'thinking' loop), where H and L cooperate to refine a shared latent state and progressively improve the output prediction .   Importantly,   HRM can decide when to halt via a learned halt signal , so it computes until it believes the solution is stable . This architecture was trained from scratch on 1,000 ARC tasks (no external data or pretraining), yet it achieved around 40-41% accuracy on ARC's eval set - a striking result given its relatively small   size   (27M   parameters)   and   limited   training   data.   The   ARC   organizers   verified   HRM's performance on a hold-out set, confirming it solved ~32% of tasks there .   While not beating hybrid systems, HRM showed that even a modest neural net without pre-training can learn to solve a substantial portion of ARC by leveraging an internal iterative reasoning process. Notably, HRM's outputs   are   produced via   transduction   (direct   prediction) rather   than   via   an   interpretable program, distinguishing it from inductive solvers . The creators attribute its power to 'significant computational depth' from the recurrent hierarchy, which allows it to simulate multi-step reasoning in a single forward pass . HRM's emergence caused quite a buzz, as it hinted that specialized architectures could tackle reasoning tasks far more efficiently than gargantuan LLMs, given the right inductive biases . · 34 35 35 35 36 37 38 39 40 41 42 43
- Compression and Minimum Description Length: Pushing test-time learning to the extreme, Liao &amp; Gu (2024) asked: Can we solve ARC with no prior training at all? They introduced CompressARC , a tiny 76k-parameter network that is trained from scratch on each puzzle by minimizing the description length of that task . In essence, their method treats the combination of the model parameters and the task solution as a message to be compressed - it performs gradient descent on the model such that it can 'encode' the given examples as concisely as possible, thereby discovering the underlying rule. This approach, akin to Solomonoff induction or MDL-based inference, yielded an astonishing result: around 20% of ARC puzzles were solved with zero pretraining . In other words, even a randomly initialized neural network can learn to solve one ARC task in isolation , if guided by a suitable objective (here, lossless compression of the example data). The tasks solved by CompressARC included a diverse set of ARC puzzles, suggesting that minimizing description length is a viable path to induce abstract rules without any outside knowledge . While 20% is far from the top scores, the implication is profound: it demonstrates a form of one-shot generalization in a deep network purely through transductive adaptation. The work underscores the connection between data compression and intelligence, and provides an existence proof that pretraining on massive datasets (e.g. as LLMs do) is not the only way to achieve non-trivial generalization . · 44 44 45 46 47
- Large LLMs with Prompting: Even without gradient-based adaptation, researchers have tested prompt-engineering techniques to coax generalization from large models. For example, giving GPT-4 a very detailed prompt that includes the ARC task as a textual description or a specially encoded ·

sequence can sometimes produce the correct output grid. Techniques like chain-of-thought (CoT) prompting (asking the model to reason step by step) and self-consistency (sampling multiple CoT outputs and voting) have been explored, inspired by their success in other reasoning benchmarks . These methods can solve certain easier ARC tasks - especially ones that resemble algorithmic puzzles - but overall performance remains low, often in the single digits percent. A study by BoberIrizar et al. (2024) found that even the largest LLMs tend to solve a different subset of tasks compared to program-synthesis solvers, typically those requiring simple pattern completions or perceptual mapping . When they combined an LLM (with a custom grid-encoding prompt) with a symbolic solver,   the   ensemble   did   better   than   either   alone .   This   indicates   that   LLMs   have complementary strengths : they act as powerful pattern recognizers or imitators on some tasks, but lack   reliability   on   others.   Notably,   straightforward   LLM   prompting   without   any   fine-tuning   was essentially worthless on ARC in 2020 (as noted, GPT-3 scored 0), but by 2023-2024, improved models and prompts began scoring into double digits . OpenAI's early-2024 ARC attempt (codename O3) reportedly leveraged enormous compute to brute-force ARC with GPT-4, achieving around 20%30% on the hidden test - but this was done outside the competition constraints and highlighted the inefficiency of using unguided LLM sampling . The consensus is that pretrained models alone are not enough for ARC; they must be either augmented with search or adapted to each task to make real progress . 48 49 49 50 51 52 30 53

Overall, transductive approaches have made striking advances in the last two years, chiefly by introducing adaptation at inference time . Unlike the static neural models of the past, today's best transductive solvers learn during the test phase - whether by a few gradient steps (TTT), iterative self-refinement (HRM), or even a full re-training on the spot (CompressARC). These methods excel especially on tasks that are hard to explicitly program. As Li et al. (2024) observed, a neural direct model often succeeds on 'fuzzier' patternrecognition tasks (e.g. those involving complex visual completion or repetition) that stumped symbolic solvers,   whereas   the   symbolic/inductive   methods   were   better   at   precise   multi-step   logic .   This complementarity sets the stage for hybrid approaches. 54

## Hybrid and Integrated Approaches

Given the respective strengths of induction and transduction, the frontier of ARC research has gravitated toward hybrid systems that combine both. The ARC Prize 2024 competition underscored this: 'all   top scores   ...   use   a   combination   of   transduction   and   induction' ,   and   indeed   the   highest   performers   were ensembles . A clear example is the work of Li et al. (2024), which won a Best Paper award. They trained two neural models on generated ARC-like tasks - one model was trained in an inductive mode (to output a latent program that could be executed) and another in a transductive mode (to output the answer directly) . Even though both models had the same architecture and saw the same training tasks , they generalized differently on the ARC evaluation: the inductive model excelled at tasks requiring compositional logic or precise   arithmetic ,   while   the   transductive   model   succeeded   on   tasks   requiring intuitive   pattern recognition or 'perceptual' rules .   By ensembling the two ,   checking if either model's output was correct, they were able to solve significantly more tasks than either alone - approaching human-level coverage on the public benchmark . This study not only provided a systematic analysis of induction vs. transduction, but also delivered a practical blueprint: use different paradigms in parallel and combine their outputs. As the authors put it, 'inductive and transductive models solve different kinds of problems' ,   so   a multipronged approach is necessary for broad generality . 11 54 55 56 51

Ensembling has indeed been a key to recent progress. The ARC-Challenge 2023 winners ('MindsAI' and others)   began   using test-time   training   +   program   search   ensembles ,   and   by   2024   the   first-place   team (' ARChitects ') did the same . They fine-tuned a large model on-the-fly for each task (a transductive element) and ran a symbolic program solver in tandem, then used heuristics to decide which solver's answer to trust or whether to merge their outputs. Another top approach by Akyürek et al. combined their 8B TTT model with an existing program-generation solver, achieving the then-best score of ~62.8% on the public set . The lesson is that diversity of methods yields a higher overall solve rate - an observation quantified by Bober-Irizar &amp; Banerjee (2024) in their ensemble analysis .   They found that certain tasks unsolvable by any program-based method could be solved by an LLM, and vice versa . Thus, combining a symbolic, a neural, and other specialized solvers produced the best coverage .   This mirrors how humans approach ARC - people use a variety of strategies, from visual intuition to logical deduction . In fact, a recommended direction for the future is to study human problem-solving on ARC and emulate the 'diverse strategies humans use' to inform AI designs . 57 58 59 60 61 60 62 60 63 64 64

Another hybrid direction is using neural nets within the search process (not just alongside it). The technical report by Chollet et al. (2024) highlights 'deep learning-guided program synthesis' as a primary paradigm . This involves, for example, training a neural network to decide which branch of the search tree to explore first, or to predict which DSL primitives are likely relevant for a given task description. AlphaCode-style techniques or learned heuristics can dramatically cut down search time by focusing on plausible programs. While much of the competition focus has been on end-to-end learning or TTT, some teams and companies (e.g. NDEA) are explicitly pursuing this guided search approach as a route to AGI .   The   idea   is analogous to how Deep Blue or AlphaGo used learned evaluations to guide brute-force search - here, a specialized 'ARC-net' could prune the vast space of programs. Preliminary results (e.g. ConceptARC's use of a CNN to rate program outputs , or LLMs generating program refinements ) are promising, but a fully integrated deep-guided ARC solver is still in development. 19 65 66 26 25

Efficiency and Generality: It's worth noting that ARC-AGI research also emphasizes efficiency - solutions are expected to solve tasks under strict time/compute limits (no $1M brute force run) . Inductive approaches naturally incorporate this by seeking the shortest program (a compressed solution), whereas transductive approaches aim to learn quickly from minimal data. Many hybrid solutions incorporate fast heuristics (e.g.   pre-checking trivial transformations) before deploying heavier machinery. This focus on efficient generalization is in line with Chollet's definition of intelligence: 'skill acquisition efficiency' on new tasks . In other words, the goal is not merely to solve ARC by any means, but to do so in a way that scales   like   human   problem-solving   -   with   reasonable   computation   and   without   task-specific   tuning. Progress is measured not just by accuracy but also by how broadly and efficiently the approach can handle the space of tasks. 67 68 69 67

## Conclusion

Five years of intensive research on the ARC-AGI benchmark have yielded a rich landscape of solutions, yet no single method has cracked the full challenge . We have seen purely symbolic program searches reach   ~30-40%   success,   purely   neural   networks   (with   clever   adaptation)   also   reach   ~40-50%,   and combinations pushing above 50% on the original ARC dataset . This is still well below human near6 70 11

perfect performance, indicating that fundamental advances are needed . The literature suggests a few takeaways for the path forward:

- No Silver Bullet: ARC is adversarially diverse; methods that work for one class of tasks fail for others .   The   most robust systems employ multiple representations and reasoning mechanisms, reflecting the consensus that general intelligence requires a toolkit of strategies . · 71 51 72 64
- Learning to Learn (Fast): Techniques that enable rapid adaptation to a new task - be it test-time gradient updates, meta-learning, or dynamic reasoning loops - have proven essential .   A promising direction is improving these techniques so that models can acquire the task concept even more efficiently (with fewer updates, or in a more human-like fashion). · 33 32
- Bridging Perception and Symbolism: The integration of neural pattern recognition with symbolic abstraction is a recurring theme. Future approaches may blur the line even more, for example, neural networks that internally execute a form of program (as in neural execution engines), or symbolic systems that call on neural nets for perceptual subroutines. The induction-transduction dichotomy is likely to converge ,   yielding models that can choose to compile knowledge into a program when appropriate or compute an answer directly when that's easier . · 9 40
- New Benchmarks and Data: Recognizing some shortcomings in the original ARC (e.g. potential overfitting to 100 hidden tasks), Chollet and colleagues are releasing ARC-AGI-2 , a larger and more calibrated dataset, to continue the challenge . This will provide fresh puzzles and reduce the chance of solutions exploiting idiosyncrasies of ARC-1. Important papers in the next few years will no doubt pivot to ARC-AGI-2 results, testing whether the lessons from ARC-1 carry over and whether new techniques emerge. · 73 13

In conclusion, the ARC-AGI benchmark has catalyzed research at the intersection of cognitive reasoning, program synthesis, and meta-learning. From hand-coded solvers that brute-forced half the tasks to brain-inspired nets that learn to think with minimal data , the progress has been remarkable. Yet ARC remains a formidable trial - as of late 2024, a gap remains between the best AI (55.5% on ARC-1) and the finish line of 85% (the nominal 'AGI' threshold) . The literature of the last five years suggests that closing this gap will require further unifying of approaches - harnessing the precision of symbolic induction, the flexibility of neural transduction, and possibly new ingredients like analogical reasoning, causal inference, or richer world models. By studying and combining the 'inductive' and 'transductive' camps , researchers inch closer to an AI that efficiently learns any new task it encounters. ARC-AGI continues to be a driving 'north star' for such research, and the coming years will be exciting as the community strives to finally surpass the human benchmark on these abstract puzzles . 13 38 35 74 7 8 70 11

Sources: The review references key contributions including Chollet's foundational ARC paper , the ARC Prize 2024 Technical Report , and recent research such as inductive logic programming for ARC , neural program synthesis in Scientific Reports 2024 , test-time training approaches , the Hierarchical Reasoning Model , and others as cited throughout. These works collectively map the evolution of ideas aiming to solve the ARC-AGI benchmark. 75 6 51 21 17 32 38 35

- ARC Prize 2024: Technical Report https://arxiv.org/html/2412.04604v2 1 2 3 4 5 6 7 8 9 10 11 13 50 51 58 59 70 73 74
- ConceptSearch: Towards Efficient Program Search Using LLMs for Abstraction and Reasoning Corpus (ARC) 12 15 16 23 24 25 26 27 28 29 30

https://arxiv.org/html/2412.07322v2

- ARC-AGI 2025: A research review https://lewish.io/posts/arc-agi-2025-research-review 14 33 52 53 65 66 67 68 69
- Neural networks for abstraction and reasoning | Scientific Reports https://www.nature.com/articles/s41598-024-73582-7? error=cookies\_not\_supported&amp;code=a326e542-9946-4137-937d-01e706d8bf8a 17 49 60 61 62 63 64 72 75
- cbmm.mit.edu https://cbmm.mit.edu/sites/default/files/publications/CBMM%20Memo%20113.pdf 18
- arcprize.org https://arcprize.org/media/arc-prize-2024-technical-report.pdf 19 20 57
- [2405.06399] Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus 21 22

https://arxiv.org/abs/2405.06399

- The Surprising Effectiveness of Test-Time Training for Abstract Reasoning https://arxiv.org/html/2411.07279v1 31 32 48

The Hidden Drivers of HRM's Performance on ARC-AGI 34 35 36 37 38 39 40 43

https://arcprize.org/blog/hrm-analysis

- [2506.21734] Hierarchical Reasoning Model 41 42

https://arxiv.org/abs/2506.21734

- [2512.06104] ARC-AGI Without Pretraining https://www.arxiv.org/abs/2512.06104 44 45 46 47
- [2411.02272] Combining Induction and Transduction for Abstract Reasoning https://arxiv.org/abs/2411.02272 54 55 56 71