# Hierarchical Reasoning Model (HRM)

**Paper**: arXiv 2506.21734v3
**Authors**: Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, Yasin Abbasi Yadkori (Sapient Intelligence, Singapore & Tsinghua University)
**Year**: 2025

## TL;DR
Brain-inspired hierarchical recurrent architecture with two-level processing (slow high-level planning + fast low-level execution) that achieves 40.3% on ARC-AGI-1 with only 27M parameters and 1000 training examples, without pre-training or chain-of-thought supervision.

## What They Did
- Introduced Hierarchical Reasoning Model (HRM) with two coupled recurrent modules operating at different timescales
- Developed hierarchical convergence mechanism where fast L-module repeatedly converges within cycles, reset by slow H-module
- Proposed one-step gradient approximation (O(1) memory vs BPTT's O(T)) based on Deep Equilibrium Models and Implicit Function Theorem
- Implemented deep supervision: each forward segment gets independent supervision, gradients don't propagate between segments
- Integrated adaptive computation time (ACT) using Q-learning to dynamically adjust computational steps based on task difficulty
- Achieved near-perfect performance on complex symbolic tasks (Sudoku-Extreme, Maze-Hard) that completely failed state-of-the-art CoT models

## Key Mechanism

**Architecture**: Four learnable components:
- Input network f_I: projects input to working representation
- Low-level module f_L: fast recurrent Transformer for detailed computations
- High-level module f_H: slow recurrent Transformer for abstract planning
- Output network f_O: extracts prediction from H-module state

**Hierarchical Convergence**:
- During each cycle (T timesteps), L-module updates conditioned on its previous state, H-module's current state (fixed during cycle), and input
- L-module converges to local equilibrium dependent on current H-state
- After T steps, H-module updates once using L-module's final state
- H-update establishes new context, "restarting" L-module toward different equilibrium
- Effective depth: NT steps (N cycles × T timesteps) vs standard RNN's rapid convergence

**Training**:
- Multiple forward passes (segments), each with M_max limit
- Deep supervision: each segment m produces prediction ŷ_m and gets loss L_m
- States detached between segments (1-step gradient approximation)
- ACT halting: Q-head predicts Q(halt) and Q(continue) values, chooses action via epsilon-greedy
- Reward: binary correctness for halt action, 0 for continue

**Inference-time Scaling**: Can increase M_max during inference for better performance without retraining (especially effective for Sudoku, minimal gains for ARC)

## Results

**ARC-AGI-1** (trained on ~1000 examples):
- HRM: 40.3% accuracy
- o3-mini-high: 34.5%
- Claude 3.7 8K context: 21.2%
- Direct pred (8-layer Transformer, same size): ~18%

**Sudoku-Extreme** (1000 training examples, mean difficulty: 22 backtracks):
- HRM: near-perfect accuracy
- All CoT models: 0% (complete failure)
- Direct pred baseline: 0%

**Sudoku-Extreme-Full** (3.8M training examples):
- HRM: 98%+ accuracy
- Direct pred (8-layer Transformer): 16.9%

**Maze-Hard** (30×30 mazes, 1000 examples):
- HRM: near-perfect accuracy
- All CoT models: 0% (complete failure)
- Vanilla Transformer (175M params, 1M examples, pass@64): <20% (from Lehnert et al.)

**Context/Efficiency**:
- 27M parameters vs GPT-scale models
- 30×30 grid context (900 tokens) vs 8K+ context windows
- 1000 training examples vs massive pre-training

## Key Quotes

> "The fixed depth of standard Transformers places them in computational complexity classes such as AC^0 or TC^0, preventing them from solving problems that require polynomial time... LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning."
- Context: Motivating why deep reasoning requires going beyond fixed-depth architectures

> "Hierarchical convergence... allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step."
- Context: Core mechanism distinguishing HRM from standard RNNs that converge too early

> "With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes."
- Context: Demonstrating data efficiency compared to LLM scaling paradigm

> "The high-to-low PR ratio in HRM (z_H/z_L ≈ 2.98) closely matches that measured in the mouse cortex (≈ 2.25). In contrast, conventional deep networks often exhibit neural collapse, where last-layer features converge to a low-dimensional subspace."
- Context: Brain correspondence - HRM spontaneously develops hierarchical dimensionality organization similar to biological cortex (Participation Ratio analysis)

> "Like earlier neural reasoning algorithms including the Universal Transformer, HRM is computationally universal when given sufficient memory and time constraints... it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers."
- Context: Theoretical capabilities - Turing-completeness

## Relevance to ETRM

HRM is the direct predecessor that TRM/ETRM simplified and extended:

**What TRM Kept from HRM**:
- Recurrent reasoning architecture with effective computational depth
- Brain-inspired hierarchical processing principles
- Adaptive computation (though TRM uses outer loop refinement instead of Q-learning ACT)
- Success on ARC-AGI benchmark without pre-training

**What TRM Simplified**:
- Removed the dual H-module/L-module architecture → single recurrent module
- Removed Q-learning based ACT → simpler outer loop with fixed steps
- Removed deep supervision → standard training
- Made architecture more minimalist while maintaining strong performance

**What ETRM Added**:
- Demo encoder to replace learned puzzle embeddings
- True generalization to unseen puzzles (not in training set)
- More interpretable single-level recurrence

Understanding HRM explains:
1. Why TRM uses recurrence (effective depth for reasoning)
2. Why outer loop exists (parallel to HRM's hierarchical updates)
3. Origins of the brain-inspired design philosophy
4. The simplification decisions made in TRM (trading some theoretical elegance for practical simplicity)

## Limitations/Gaps

**From the paper itself**:
- Theoretical grounding exists (Turing-completeness) but practical depth still limited by training stability
- 1-step gradient approximation is an approximation - full IFT gradient might perform better but is computationally expensive
- Q-learning ACT adds complexity and can be unstable (though they claim stability via architecture properties)
- Still requires augmentation and voting for ARC-AGI (1000 augmented variants per test puzzle)
- Minimal gains from inference-time scaling on ARC (suggests solutions are relatively shallow)

**From analysis papers** (referenced in proposal context):
- Like TRM, likely relies heavily on memorization of training puzzle patterns
- The "generalization" is within-distribution (augmentations of seen puzzles) not true out-of-distribution
- Evaluation puzzles' demonstrations are seen during training (similar to embedding mode issue)
- Success on ARC-AGI-1 may not transfer to fundamentally novel reasoning patterns

**Architectural Complexity**:
- Dual-module design harder to understand and debug than single module
- Deep supervision + Q-learning adds training complexity
- Not clear if hierarchical architecture is necessary vs sufficient for performance
- TRM's simpler architecture achieves comparable results, suggesting HRM may be over-engineered

**Limited Analysis**:
- Visualization shows different strategies emerge (DFS for Sudoku, exploration for Maze) but mechanism unclear
- No ablation studies on H-module vs L-module contribution
- Brain correspondence (PR ratio) is correlational not causal
- Doesn't explain what abstract patterns are actually learned
