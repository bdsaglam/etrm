# Less is More: Recursive Reasoning with Tiny Networks (TRM)

**Paper**: arXiv 2510.04871v1
**Authors**: Alexia Jolicoeur-Martineau (Samsung SAIL Montréal)
**Year**: 2025

## TL;DR
TRM simplifies HRM's recursive reasoning approach by using a single tiny 2-layer network (7M params) instead of two 4-layer networks (27M params), achieving 45% on ARC-AGI-1 and 8% on ARC-AGI-2 by recursively improving predictions through deep supervision without requiring fixed-point theorems or biological justifications.

## What They Did
- Simplified Hierarchical Reasoning Model (HRM) by eliminating complex theoretical requirements (fixed-point theorem, biological arguments)
- Replaced two 4-layer networks (fL and fH) with a single 2-layer network
- Reinterpreted dual-state design: zH → y (current solution), zL → z (latent reasoning), x (input question)
- Removed need for extra forward pass in Adaptive Computational Time (ACT) halting mechanism
- Added Exponential Moving Average (EMA) for training stability
- Introduced attention-free MLP architecture for small fixed-context tasks (Sudoku)
- Achieved state-of-the-art on: Sudoku-Extreme (87%), Maze-Hard (85%), ARC-AGI-1 (45%), ARC-AGI-2 (8%)

## Key Mechanism

### Dual-State Design (Simplified from HRM)
**Original HRM interpretation**: Two hierarchical latents (zL, zH) operating at different frequencies based on biological brain processing.

**TRM reinterpretation** (much simpler):
- **x**: Input question (embedded)
- **y**: Current predicted solution (previously called zH)
- **z**: Latent reasoning feature (previously called zL)

The model recursively:
1. Improves latent reasoning: `z = net(x, y, z)` (n=6 times)
2. Refines answer: `y = net(y, z)` (once)

### Recursive Reasoning Process
```python
def latent_recursion(x, y, z, n=6):
    for i in range(n):  # n=6 latent reasoning steps
        z = net(x, y, z)
    y = net(y, z)  # refine output answer
    return y, z

def deep_recursion(x, y, z, n=6, T=3):
    # T-1 times without gradients (improve y and z)
    with torch.no_grad():
        for j in range(T-1):
            y, z = latent_recursion(x, y, z, n)
    # Once with gradients (backprop through full recursion)
    y, z = latent_recursion(x, y, z, n)
    return (y.detach(), z.detach()), output_head(y), Q_head(y)
```

**Effective depth**: T × (n+1) × n_layers = 3 × 7 × 2 = 42 layers (vs 384 in HRM)

### Deep Supervision
- Up to Nsup=16 supervision steps
- Each step: use previous (y, z) as initialization (detached from computational graph)
- Model learns to progressively improve answer across supervision steps
- Acts as residual connections emulating very deep networks

### Adaptive Computational Time (ACT) - Simplified
**HRM**: Required 2 forward passes (halting loss + continue loss via Q-learning)

**TRM**: Only 1 forward pass
- Single Binary Cross-Entropy loss: "has correct solution been reached?"
- Early stopping when halting probability > threshold
- Only used during training; full Nsup=16 steps at test time

### Puzzle_id Embedding (The Critical Limitation)
```python
# For ARC-AGI specifically:
# Each puzzle at each data augmentation gets a specific embedding
puzzle_embedding = embeddings[puzzle_id]  # shape: [1, D=512]
x = input_embedding(x_input) + puzzle_embedding
```

**Key quote from paper (page 333)**:
> "For ARC-AGI, each puzzle (containing 2-3 training examples and 1-2 test examples) at each dataaugmentation is given a specific embedding of shape [0, 1, D]"

**At test-time**:
> "at test-time, the most common answer out of the 1000 data augmentations is given as answer"

This means:
- Every augmented version of a puzzle has a learned embedding in the matrix
- Embedding matrix includes BOTH training and evaluation puzzles
- Gradients flow through evaluation puzzle embeddings during training
- Model cannot generalize to truly unseen puzzles (not in embedding matrix)

## Results

### State-of-the-Art Performance (with 0.01% parameters of LLMs)

| Benchmark | TRM-Att (7M) | HRM (27M) | Improvement |
|-----------|--------------|-----------|-------------|
| Sudoku-Extreme | 74.7% (87.4% MLP) | 55.0% | +32.4% (MLP) |
| Maze-Hard | 85.3% | 74.5% | +10.8% |
| ARC-AGI-1 | 44.6% | 40.3% | +4.3% |
| ARC-AGI-2 | 7.8% | 5.0% | +2.8% |

**Comparison to LLMs** (ARC-AGI-1):
- TRM-Att (7M): 44.6%
- Gemini 2.5 Pro (32K): 37.0%
- o3-mini-high: 34.5%
- Deepseek R1 (671B): 15.8%

### Training Efficiency
- Sudoku-Extreme: <36 hours on 1 L40S (40GB RAM)
- Maze-Hard: <24 hours on 4 L40S
- ARC-AGI: ~3 days on 4 H100 (80GB RAM)

### Why "Less is More" Works
1. **2 layers optimal**: Adding layers decreased generalization due to overfitting on small data
2. **Single network**: Shared network for both z and y updates (task specified by input presence)
3. **Smaller = less overfitting**: With ~1000 training examples, tiny networks + deep recursion bypass overfitting
4. **Deep supervision prevents need for BPTT**: More biologically plausible and memory-efficient

## The Puzzle_id Problem

**This is the fundamental limitation we address in ETRM.**

### How TRM Uses Puzzle_id
1. **Embedding matrix creation**: For ARC-AGI, create learned embeddings for every puzzle × augmentation combination
2. **Training**: Input x gets puzzle-specific embedding added: `x = embed(x_input) + puzzle_emb[puzzle_id]`
3. **Evaluation**: Same puzzle_id lookup for test inputs from that puzzle

### Why This is Memorization, Not Generalization

**Quote from paper explaining the issue** (implicit in setup):
> "For ARC-AGI, each puzzle (containing 2-3 training examples and 1-2 test examples) at each dataaugmentation is given a specific embedding"

This means:
- **Embedding matrix includes evaluation puzzles**: Even though eval puzzles' test queries aren't in training data, their puzzle_id embeddings ARE trained
- **Task identity is given**: Model doesn't learn "what is this puzzle asking?" from demos—it's told via the embedding
- **Cannot handle new puzzles**: Any puzzle without an embedding matrix entry cannot be solved
- **Not truly few-shot**: Model isn't learning from the 2-3 demonstration examples; it's using a learned task-specific embedding

### Data Augmentation Amplifies the Issue
- 1000 augmentations per puzzle → 1000 different puzzle_id embeddings
- Each augmentation: same color permutation + rotation for ALL examples (demos + test)
- Embedding matrix size: ~876,406 entries (560 training + 400 eval puzzles × augmentations)
- Test-time voting: Predict on all 1000 augmented versions, take majority vote

### The Generalization Claim
Paper claims superiority over LLMs, but:
- LLMs see NO task-specific training on ARC-AGI puzzles
- TRM has learned embeddings for EVERY evaluation puzzle
- This is closer to "interpolation" than "generalization"

**The key limitation**: Cannot solve a brand new ARC-AGI puzzle that wasn't in the training dataset, even if given demonstration examples, because there's no embedding for it.

## Key Quotes

> "HRM is a novel approach using two small neural networks recursing at different frequencies. [...] We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers."
- Context: Main contribution—simplifying while improving performance

> "The fact of the matter is: zH is simply the current (embedded) solution. [...] On the other hand, zL is a latent feature that does not directly correspond to a solution, but it can be transformed into a solution by applying zH ← fH(x, zL, zH)."
- Context: Explaining why 2 features (not hierarchical, just y=solution and z=reasoning)

> "Thus, we need both y and z separately, and there is no apparent reason why one would need to split z into multiple features."
- Context: Justifying dual-state design without complex biological arguments

> "For ARC-AGI, each puzzle (containing 2-3 training examples and 1-2 test examples) at each dataaugmentation is given a specific embedding of shape [0, 1, D] and, at test-time, the most common answer out of the 1000 data augmentations is given as answer."
- Context: **THIS IS THE CRITICAL QUOTE** explaining puzzle_id embeddings and voting mechanism

> "Deep supervision consists of reusing the previous latent features (zH and zL) as initialization for the next forward pass. This allows the model to reason over many iterations and improve its latent features (zL and zH) until it (hopefully) converges to the correct solution."
- Context: Explaining the core training mechanism that replaces BPTT

> "Surprisingly, we found that adding layers decreased generalization due to overfitting. In doing the opposite, decreasing the number of layers while scaling the number of recursions (n) proportionally [...] we found that using 2 layers (instead of 4 layers) maximized generalization."
- Context: Explaining why tiny networks work better on small data

## Relevance to ETRM

**TRM is the base architecture we build upon.** Our ETRM project directly addresses its fundamental limitation:

### What We Keep from TRM
- Dual-state design (y=solution, z=reasoning)
- Recursive reasoning with deep supervision
- Single tiny network architecture
- Training approach (AdamW, EMA, ACT, stable-max loss)
- Data augmentation and voting mechanism

### What ETRM Changes
**Replace puzzle_id embedding with an encoder:**

```python
# TRM (memorization):
x = input_embedding(x_input) + puzzle_emb[puzzle_id]

# ETRM (true few-shot learning):
demo_encoding = encoder(demo_inputs, demo_labels)  # Learn from demos
x = input_embedding(x_input) + demo_encoding
```

**Why this matters**:
- **True generalization**: Can solve puzzles never seen during training
- **Learn from demonstrations**: Model must understand what the puzzle asks from the 2-3 examples
- **No task memorization**: No puzzle-specific parameters in the model
- **Fair comparison to LLMs**: Same few-shot setting

### Architecture Integration
ETRM = TRM decoder + Demo Encoder
- Encoder: Processes (demo_inputs, demo_labels) → task representation
- TRM decoder: Takes encoded task + test_input → prediction
- Both trained end-to-end with deep supervision

## Limitations/Gaps

### Acknowledged in Paper
1. **No theory for why recursion helps**: "the question of why recursion helps so much compared to using a larger and deeper network remains to be explained; we suspect it has to do with overfitting"
2. **Architecture choices not universal**: MLP works for Sudoku, fails on larger grids
3. **Supervised only, not generative**: "can only provide a single deterministic answer"
4. **Scaling laws needed**: "Different problem settings may require different architectures or number of parameters"

### Critical Unacknowledged Limitation
**Puzzle_id embedding = memorization**, not generalization:
- Cannot solve truly new puzzles
- Embedding matrix includes evaluation puzzles
- Not a fair comparison to LLMs (which see no task-specific training)
- Violates the spirit of few-shot learning on ARC-AGI

### Other Gaps
1. **No ablation on embedding matrix size**: What happens with only training puzzles?
2. **No analysis of embedding learned structure**: What do puzzle embeddings capture?
3. **No attempt at true few-shot**: Why not try learning from demonstrations?
4. **Limited discussion of data augmentation role**: 1000× augmentation is massive—how much does this contribute vs architecture?

### Failed Ideas (from paper's appendix)
- Mixture-of-Experts (too much capacity)
- Partial backpropagation (k<n steps)
- Removing ACT entirely (too much time per sample)
- Weight tying input/output (too constraining)
- TorchDEQ fixed-point iteration (slower, worse generalization)

---

**Bottom line**: TRM is an excellent architecture for recursive reasoning with deep supervision on small data, but its reliance on puzzle_id embeddings fundamentally limits it to interpolation rather than generalization. ETRM addresses this by replacing the embedding lookup with an encoder that learns from demonstration examples.
