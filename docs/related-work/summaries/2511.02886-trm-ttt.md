# Test-time Adaptation of Tiny Recursive Models

**Paper**: arXiv 2511.02886v1
**Authors**: Ronan McGovern (Trelis LTD)
**Year**: 2025

## TL;DR
This paper demonstrates that pre-training a 7M parameter TRM model on public ARC tasks enables efficient fine-tuning on competition tasks within compute limits. Starting from a model achieving 10% on public evaluation tasks, full fine-tuning (not LoRA or embedding-only) during competition reached 6.67% on semi-private tasks in just 12.5k gradient steps.

## What They Did
- Pre-trained TRM models on public ARC tasks (1,280 tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXMs)
- Tested three pre-training variants: (1) TRM paper replication (100k epochs), (2) expanded data for 200k epochs, (3) filtered hard data for 1M epochs
- Fine-tuned pre-trained models at competition time within compute limits (4xL4 GPUs for 12 hours)
- Compared four post-training methods: full fine-tuning, embeddings-only, embeddings-then-full, LoRA+embeddings
- Achieved 6.67% on ARC AGI II semi-private evaluation tasks (competition submission)

## Key Mechanism

**Pre-training Phase** (no compute limit):
- Train 7M parameter recursive transformer on 1,280 public ARC tasks
- Train for 700k+ optimizer steps using global batch size of 768
- Achieve ~10% accuracy on ARC AGI II public evaluation set
- Each task gets a learned embedding (512-dim × 16), creating a 500M+ parameter embedding matrix

**Test-time Fine-tuning Phase** (competition environment - 4xL4 GPUs, 12 hours):
- Initialize from pre-trained checkpoint
- Create new task embeddings for competition tasks (initialized to mean of pre-trained embeddings)
- Fine-tune on train examples from test tasks for only 12.5k-15k optimizer steps
- Use global batch size of 384 (half of pre-training, due to L4 memory limits)
- Double learning rates to compensate for smaller batch size
- **Full fine-tuning performs best** - updating both embeddings AND trunk parameters

**Why It Works**:
- Pre-training shapes the neural network's parameters to accelerate learning on new, unseen tasks
- Requires ~32x less compute than training from scratch (15k vs 750k optimizer steps)
- Evidence of meaningful inter-task learning and shared concepts across ARC puzzles

## Results

**Pre-training Performance**:
- TRM paper replication (100k epochs): 10% pass@2 on ARC AGI II public eval (120 tasks)
- Extended data (200k epochs): Similar performance but smaller hold-out set
- Filtered hard data (1M epochs, 230 tasks): Lower performance, suggesting dataset diversity matters

**Post-training Comparison** (on ARC AGI I → ARC AGI II transfer):
- **Full fine-tuning**: Best performance (highest pass@2 improvement)
- **Embeddings-then-full**: Similar to full fine-tuning (trains embeddings for 1/4 of steps, then full)
- **Embeddings-only**: Near zero performance on held-out tasks
- **LoRA + embeddings**: Low performance, worse than full fine-tuning

**Competition Results** (semi-private ARC AGI II tasks):
- TRM paper replication → 6.67% (best)
- Expanded data, 200k epochs → 4.25%
- Filtered hard data, 1M epochs → 1.27%

**Key Findings**:
- Pre-training from scratch in competition environment yields ~0% (only 10-20k steps possible)
- Fine-tuning from pre-trained model trends toward scratch performance but with 32x less compute
- Stochastic variance is high: same model re-submitted ranged from 3.33% to 6.67%
- More training data diversity appears better than longer training on harder subset

## Key Quotes

> "This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits."
- Context: Main thesis - demonstrates compute-efficient path to ARC competition entry

> "Notably, such post-training performance is achieved by full fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone."
- Context: Critical finding - unlike typical transfer learning, tiny models benefit from updating all parameters

> "Clearly - and even with independent ARC tasks - it is possible to pre-shape the neural network in a manner that accelerates tuning on unseen tasks."
- Context: Evidence of inter-task learning despite tasks being designed to be independent

> "While the TRM paper model has 7M parameters in its trunk, there are 2.5 GB of parameters required to capture 1,000 augmentations of 1,000 tasks (1k tasks x 1k augs/task x 512 dimensions = 512 M parameters). As such, when encoding each task augmentation individually, the TRM is more a 500M+ parameter model than a 7M parameter model."
- Context: Reveals that task embeddings dominate parameter count (500M vs 7M)

> "Perhaps the performance gap between SLPS and TRM with post-training can be attributed to: (1) The more complete set of augmentations used by TRM... (2) The fact that SLPS searches only for the best latent... but cannot add/encode new primitives. (3) The recursive nature of the TRM's neural net..."
- Context: Comparison to Searching Latent Program Spaces - explains why full fine-tuning outperforms embedding-only search

## Relevance to ETRM

**Alternative Generalization Approach**:
- TRM+TTT (test-time training): Fine-tune at test time to adapt to new puzzles
- ETRM (encoder): Compute puzzle representation from demos without fine-tuning

**Key Differences**:
| Aspect | TRM+TTT (This Paper) | ETRM (Our Approach) |
|--------|----------------------|---------------------|
| Test-time compute | Requires gradient updates (12.5k steps) | Single forward pass through encoder |
| Generalization | Adapts through fine-tuning | Zero-shot from demo encoding |
| New puzzles | Needs training examples to fine-tune | Just needs demonstration examples |
| Deployment | Requires optimization at inference | Inference-only |

**Complementary Insights**:
- Their finding that "embeddings-only fine-tuning achieves near zero" suggests puzzle representation alone is insufficient
- But ETRM encoder learns to produce representations during pre-training (not random init)
- Their cosine similarity analysis (Fig 4-5) shows TRM doesn't learn to relate task variants - ETRM encoder might explicitly learn this
- Both approaches try to solve the same problem: how to generalize beyond the fixed embedding matrix

**Theoretical Connection**:
> "Perhaps the performance gap between SLPS and TRM with post-training can be attributed to... (2) The fact that SLPS searches only for the best latent (i.e. trains only what might be thought of as the embeddings in TRM, but not other parameters). As such, SLPS finds the best combination of pre-trained primitives, but cannot add/encode new primitives."

This suggests ETRM's demo encoder is similar to SLPS - it searches for the best representation from pre-trained primitives. However:
- ETRM encoder is trained end-to-end with the reasoning model
- ETRM encoder can learn rich compositional representations from demos
- ETRM doesn't require gradient updates at test time

**Practical Implications**:
- TRM+TTT achieves 6.67% with test-time fine-tuning
- ETRM should be compared against this as a baseline
- If ETRM can match or exceed this without test-time training, it demonstrates superior generalization
- Hybrid approach possible: use ETRM encoder for zero-shot, fall back to fine-tuning if needed

## Limitations/Gaps

**Compute Requirements**:
- Still requires expensive pre-training (48 hours on 4xH100 SXMs)
- Test-time fine-tuning requires 12.5k gradient steps (slower inference than pure forward pass)
- Cannot generalize to truly new puzzles without training examples at test time

**Embedding Matrix Issues**:
- Task embeddings still dominate parameter count (500M vs 7M trunk)
- Embeddings for task variants don't show high cosine similarity - model may not learn variant relationships
- New task embeddings initialized to mean of pre-trained embeddings (crude initialization)

**Evaluation Challenges**:
- High stochastic variance (3.33% to 6.67% on same model)
- Limited public evaluation data (only 120 ARC AGI II tasks)
- Unclear what pre-training data diversity is optimal for test-time adaptation

**Unanswered Questions**:
- Why does full fine-tuning outperform embeddings-only when embeddings have 71x more parameters than trunk?
- Why doesn't the model learn to relate task variants (low cosine similarity)?
- What is the optimal split between embedding dimension and trunk dimension?
- Can explicit augmentation encoding (d4 group + color mapping) improve over per-augmentation embeddings?
- Can halting head be improved to work on evaluation tasks (currently only works on training tasks)?

**Dataset Distribution**:
- ARC AGI II training split (1000 tasks) appears out-of-distribution from evaluation tasks
- Filtering for "hard" tasks using GPT-5-mini reduced performance (1.27% vs 6.67%)
- Would benefit from more in-distribution pre-training data calibrated to evaluation difficulty

**Generalization**:
- Still requires task-specific training (even if only 12.5k steps)
- Cannot handle truly unseen puzzles without gradient updates
- Performance has not been shown to exceed pre-training from scratch (trends toward it but doesn't surpass)
