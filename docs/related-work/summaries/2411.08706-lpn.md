# Searching Latent Program Spaces (LPN)

**Paper**: arXiv 2411.08706v3
**Authors**: Matthew V. Macfarlane, Clément Bonnet (equal contribution)
**Affiliation**: University of Amsterdam
**Year**: 2024

## TL;DR
LPN learns a continuous latent space of implicit programs and performs gradient-based search at test time to adapt to novel tasks. On ARC-AGI, LPN doubles its OOD performance (7.75% → 15.5%) by scaling test-time compute, outperforming test-time training (TTT) at moderate compute budgets while using only a 178M-parameter model trained on re-ARC.

## What They Did
- Introduced **Latent Program Network (LPN)**: encoder-decoder architecture with bottleneck latent space
- Encoder maps demo I/O pairs to latent program representations (variational posterior)
- Decoder executes latent programs to generate outputs from new inputs
- **Test-time gradient search** refines latent programs to better explain given specifications
- Trained 178M-param model on re-ARC dataset for 100k steps (2 days on TPU v4-32)
- Achieved 15.25% on ARC-AGI eval set with moderate compute (2E+13 FLOPs)
- Demonstrated effective OOD generalization and specification size scaling

## Key Mechanism

### Architecture (3 Components)
1. **Encoder** `q_φ(z|x,y)`: Maps I/O pairs → latent distribution (mean pooling for permutation invariance)
2. **Latent Optimization**: Gradient ascent in latent space to maximize decoder likelihood
   - Initialize: `z₀ = mean(encoder outputs for all demo pairs)`
   - Refine: `z' ← z + α·∇_z Σ log p_θ(y_i|x_i, z)` (system 2 thinking)
3. **Decoder** `p_θ(y|x,z)`: Generates output pixel-by-pixel given input and latent program

### Training Procedure
- Reconstruct each output `y_i` using all OTHER pairs `{(x_j, y_j)}_{j≠i}` (prevents shortcut learning)
- Loss: `L = L_rec + β·L_KL` (variational objective with KL to prior)
- Key insight: Train with gradient ascent awareness (Grad 1 mode) to optimize latent space for test-time search
- Most efficient: Train 95k steps in Grad 0, fine-tune 5k steps in Grad 1

### Test-Time Adaptation
- Gradient ascent in **latent space only** (not all parameters like TTT)
- Much cheaper: backprop through decoder vs. entire model
- Scales well: 100 gradient steps → 99.5% on Pattern task (vs. 3.2% without search)

## Results

### ARC-AGI (Top-2 Accuracy)
| FLOPs | In-Distribution | Out-of-Distribution |
|-------|-----------------|---------------------|
| 2E+11 | 68.75% | 7.75% |
| 2E+12 | 75.95% | 10.25% |
| 2E+13 | **80.00%** | **15.25%** |
| 2E+14 | 76.25% | 15.50% |

**Key findings:**
- Doubles OOD performance via test-time search (7.75% → 15.5%)
- Outperforms TTT at low-to-moderate compute budgets
- Significantly cheaper than LLM baselines (CodeT5: 220M params, text-davinci: 175B params)
- At 2E+13 FLOPs: LPN (15.25%) > TTT (13.50%) on eval set

### Pattern Task
- LPN Grad 1 training: 8.6% → 99.5% (0 → 100 gradient steps at test)
- Handles OOD patterns: 0% → 88% with 100 gradient steps
- Scales to unseen specification sizes (trained on 3, works on 1-19)
- TTT overfits at high compute; in-context learning can't adapt

## Relevance to ETRM

**Direct parallel to our encoder-based TRM approach:**

1. **Demo encoding to latent space**: LPN encoder maps I/O pairs → latent programs, ETRM encoder maps demos → puzzle embeddings
   - Both replace learned embedding matrices with computed representations
   - Both enable generalization to unseen puzzles/programs

2. **Test-time adaptation**: LPN refines latents via gradients, ETRM could potentially do similar
   - Our current ETRM: fixed encoder forward pass
   - LPN insight: training with gradient awareness improves latent space quality

3. **Architectural similarities**:
   - Both use transformers for encoder/decoder
   - Both aggregate demos (LPN: mean pooling, ETRM: set layers or mean pooling)
   - Both condition decoder on latent representation

4. **Key differences**:
   - LPN: variational framework with KL regularization
   - LPN: explicit test-time optimization (we don't do this)
   - LPN: trains on synthetic re-ARC, we train on augmented original puzzles
   - LPN: smaller latent dim (256) vs. our embedding dim (512×16)

**Potential improvements for ETRM:**
- Could we add test-time latent refinement via gradient descent?
- Should we use variational encoder with KL loss?
- Mean pooling vs. set layers: LPN shows mean pooling works well
- Training with "search awareness" might improve encoder quality

## Key Quotes

> "By representing implicit programs in a continuous latent space, LPN allows for efficient test-time program search. LPN is more efficient than test-time fine-tuning methods... because it performs backpropagation only through a fraction of the total parameters."
- Context: Core advantage over TTT - cheaper test-time adaptation by optimizing latent space (256-dim) instead of all parameters (178M)

> "Initializing latent search with the encoder is critical for performance across all training methods. This validates the intuition that LPN can perform fast system 1-like reasoning using the encoder and then narrow down the search space during latent optimization, simulating system 2 reasoning."
- Context: Ablation showing encoder (amortized inference) + gradient search (refinement) both essential - encoder provides good starting point for search

> "LPN generalizes to specification sizes beyond those seen during training, even improving performance, unlike in-context learning, which fails to generalize without parameter fine-tuning at large specification sizes."
- Context: Mean pooling enables handling variable numbers of demos - trained on 3, works on 1-19 pairs. Highly relevant for ETRM since ARC puzzles have varying numbers of demos

> "Out of distribution, LPN doubles its performance by using latent space search at test-time (scaling FLOPs from 2e11 to 2e15)."
- Context: Test-time compute scaling is powerful for OOD generalization - 7.75% → 15.5% on ARC eval set

> "We explicitly factorize inference into three core components... First, we introduce a bottleneck that encourages the network to learn an explicit representation of programs via a compact latent space... Secondly, we introduce a method for searching this latent space to explain the given data effectively."
- Context: Architectural design philosophy - explicit program representation (like our encoder) + search (which we don't have) enables better generalization

> "Training with gradient ascent latent optimization (as detailed in Algorithm 2) incurs a compute overhead... we propose to use a small number of gradient ascent steps during training, ranging from 0 to 5 steps."
- Context: Practical training approach - train mostly without search (Grad 0), fine-tune briefly with search (Grad 1) - balances compute cost and performance

> "A limitation of this work is the limited diversity of programs on which LPN is trained. While augmentations are used during training, the distribution of programs remains narrow and restricts the potential for learning an expressive complex latent space."
- Context: Acknowledged limitation - trained only on re-ARC synthetic data, not diverse enough for truly general program space. Similar limitation may apply to ETRM

## Limitations/Gaps

1. **Limited training diversity**: Only re-ARC dataset (synthetic, in-distribution to ARC training)
   - No access to original ARC training puzzles
   - Narrow program distribution limits latent space expressiveness
   - Our ETRM trains on actual training puzzles (more diverse?)

2. **Compositional generalization**: Mixed results
   - Can sometimes compose programs seen during training
   - Many failure cases - not robust
   - Suggests discrete program representations might help

3. **TTT catches up at very high compute**: At 2E+14+ FLOPs, TTT matches or exceeds LPN
   - LPN's latent space may not be expressive enough with limited training data
   - Suggests scaling training diversity matters more than architecture

4. **No comparison to embedding-based methods**: Doesn't compare to learned puzzle embeddings (like original TRM)
   - Our work directly compares ETRM (encoder) vs. TRM (embedding)

5. **Architecture search not explored**: Uses standard transformers
   - Doesn't explore specialized architectures for spatial reasoning
   - Our work explores different encoder types (Set Transformer, Perceiver, etc.)

6. **Pattern task is toy domain**: Main insights from simple 10×10 pattern copying
   - ARC-AGI results are lower (15% vs. 99% on patterns)
   - Unclear how well insights transfer to truly complex programs

7. **No analysis of what programs are learned**: No interpretability studies
   - t-SNE plots show clustering but not what clusters mean
   - Can't verify if latent space captures meaningful program abstractions

**Gaps our ETRM work addresses:**
- Direct comparison to embedding baseline (we have TRM vs. ETRM)
- Training on actual diverse puzzles (not just synthetic re-ARC)
- Exploration of encoder architectures (Set Transformer, Perceiver, etc.)
- Analysis of true vs. encoder mode generalization

**Gaps we could explore (inspired by LPN):**
- Test-time gradient-based refinement of puzzle representations
- Variational framework with KL regularization
- Training with "search awareness" (Grad 1 fine-tuning)
- More rigorous OOD evaluation with specification size scaling
