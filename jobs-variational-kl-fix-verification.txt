# Variational Encoder KL Loss Fix Verification
# Goal: Verify that KL loss is now properly integrated into training
#
# Bug Fixed: KL loss was computed but never added to total_loss
# Fix: Modified models/losses.py to include kl_weight * kl_loss in total_loss
#
# These experiments verify:
# 1. KL loss appears in logs (train/kl_loss, train/kl_loss_weighted)
# 2. Gradients flow properly with KL regularization
# 3. Model can learn with variational bottleneck active
#
# Settings:
# - Overfit mode (32 groups) for fast validation
# - Short training (1000 steps) to quickly see if it works
# - Test multiple kl_weight values

# =============================================================================
# LPN VARIATIONAL: Test various KL weights
# =============================================================================
# LPN Paper uses kl_weight=0.0001 (1e-4) for ARC-AGI training
# We test a range around their value to find optimal

# VKL1: LPN_variational with kl_weight=0.0001 (LPN paper value - 1e-4)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="variational-kl-fix" +run_name="VKL1_lpn_var_kl0.0001_lpn_paper" arch.encoder_type=lpn_variational arch.encoder_num_layers=6 arch.loss.kl_weight=0.0001 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000

# VKL2: LPN_variational with kl_weight=0.001 (10x LPN value)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="variational-kl-fix" +run_name="VKL2_lpn_var_kl0.001" arch.encoder_type=lpn_variational arch.encoder_num_layers=6 arch.loss.kl_weight=0.001 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000

# VKL6: LPN_variational with kl_weight=0.00001 (10x smaller than LPN paper - test if stochasticity itself is the issue)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="variational-kl-fix" +run_name="VKL6_lpn_var_kl1e-5" arch.encoder_type=lpn_variational arch.encoder_num_layers=6 arch.loss.kl_weight=0.00001 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=10000

# =============================================================================
# HYBRID VARIATIONAL: Test LPN paper KL weight
# =============================================================================

# VKL5: Hybrid_variational with kl_weight=0.0001 (LPN paper value)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="variational-kl-fix" +run_name="VKL5_hybrid_var_kl0.0001_lpn_paper" arch.encoder_type=hybrid_variational arch.encoder_num_layers=4 arch.loss.kl_weight=0.0001 global_batch_size=128 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000

# =============================================================================
# CONTROL: Standard Encoder (kl_weight should be ignored)
# =============================================================================

# VKL0: Standard encoder with kl_weight=0 (baseline, should work same as before)
[x] torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_encoder_original.py --config-name cfg_pretrain_encoder_original_arc_agi_1 +project_name="variational-kl-fix" +run_name="VKL0_standard_control" arch.encoder_type=standard arch.encoder_num_layers=2 arch.loss.kl_weight=0.0 max_train_groups=32 max_eval_groups=32 epochs=20000 eval_interval=5000

# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# SUCCESS CRITERIA:
# 1. Metrics logged: train/kl_loss and train/kl_loss_weighted appear in W&B
# 2. Gradient flow: grad/encoder_norm > 0.1 (better than 0.073 before fix)
# 3. Learning: train/exact_accuracy > 20% (better than 19-23% before fix)
# 4. Variance: encoder_cross_sample_var remains high (>1.0 for variational)
#
# COMPARISON TO PRE-FIX RESULTS (from overfit experiments):
# - LPN_variational (no KL): grad=0.073, var=1.75, EM=23.4%
# - Hybrid_variational (no KL): grad=0.24, var=2.45, EM=19.5%
#
# WITH KL FIX, EXPECT:
# - Better gradient flow (regularization helps optimization)
# - Potentially better accuracy (noise now has purpose)
# - KL loss metrics visible in logs
# - Optimal kl_weight likely around 0.0001-0.001 (LPN paper uses 0.0001)
#
# NEXT STEPS AFTER VERIFICATION:
# If successful (VKL1/VKL2/VKL5 shows improvement):
# 1. Add best kl_weight to semi-final experiments
# 2. Run full-dataset training with variational encoders
# 3. Compare to hybrid_standard results
