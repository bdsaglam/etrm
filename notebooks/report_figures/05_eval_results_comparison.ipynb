{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results Comparison\n",
    "\n",
    "This notebook loads evaluation results from checkpoint directories and compares:\n",
    "- **Original TRM** (embedding-based)\n",
    "- **ETRM variants** (encoder-based: F1-F4)\n",
    "\n",
    "Evaluation on 32 held-out test puzzle groups with voting across augmentations.\n",
    "\n",
    "**Important**: This comparison includes training duration analysis to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from figure_utils import (\n",
    "    fetch_final_runs,\n",
    "    dataframe_to_markdown,\n",
    "    save_table,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Dataset sizes for epoch calculation\n",
    "EMB_DATASET_SAMPLES = 3_633_410  # embedding dataset (original TRM)\n",
    "ENC_DATASET_SAMPLES = 2_373_921  # encoder dataset (ETRM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CHECKPOINT_ROOT = Path(\"../../checkpoints\")\n\nEXPERIMENTS = {\n    \"TRM (Original)\": {\n        \"path\": CHECKPOINT_ROOT / \"Arc1concept-aug-1000-ACT-torch\" / \"pretrain_att_arc1concept_4\",\n        \"encoder_type\": \"embedding\",\n        \"description\": \"Original TRM with learned puzzle embeddings\",\n        \"batch_size\": 768,\n        \"dataset_samples\": EMB_DATASET_SAMPLES,\n        \"config_epochs\": 100000,\n    },\n    \"F1: Standard\": {\n        \"path\": CHECKPOINT_ROOT / \"etrm-final\" / \"F1_standard\",\n        \"encoder_type\": \"standard (2L)\",\n        \"description\": \"Standard transformer encoder, 2 layers\",\n        \"batch_size\": 256,\n        \"dataset_samples\": ENC_DATASET_SAMPLES,\n        \"config_epochs\": 50000,\n    },\n    \"F2: Hybrid VAE\": {\n        \"path\": CHECKPOINT_ROOT / \"etrm-final\" / \"F2_hybrid_var\",\n        \"encoder_type\": \"hybrid_var (4L)\",\n        \"description\": \"Hybrid variational encoder, 4 layers\",\n        \"batch_size\": 128,\n        \"dataset_samples\": ENC_DATASET_SAMPLES,\n        \"config_epochs\": 25000,\n    },\n    \"F3: ETRMTRM\": {\n        \"path\": CHECKPOINT_ROOT / \"etrm-final\" / \"F3_etrmtrm\",\n        \"encoder_type\": \"trm_style (recurrent)\",\n        \"description\": \"TRM-style recurrent encoder\",\n        \"batch_size\": 256,\n        \"dataset_samples\": ENC_DATASET_SAMPLES,\n        \"config_epochs\": 25000,\n    },\n    \"F4: LPN VAE\": {\n        \"path\": CHECKPOINT_ROOT / \"etrm-final\" / \"F4_lpn_var\",\n        \"encoder_type\": \"lpn_var (2L)\",\n        \"description\": \"LPN-style variational encoder\",\n        \"batch_size\": 256,\n        \"dataset_samples\": ENC_DATASET_SAMPLES,\n        \"config_epochs\": 25000,\n    },\n}\n\nprint(f\"Configured {len(EXPERIMENTS)} experiments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_eval_files(exp_path, pattern=\"eval_results_groups_32*.json\"):\n    \"\"\"Find all eval result files matching pattern in experiment directory.\"\"\"\n    return list(exp_path.glob(pattern))\n\n\ndef extract_step_from_filename(filename):\n    \"\"\"Extract step number from filename like 'eval_results_groups_32_step_174240.json'.\"\"\"\n    match = re.search(r'step_(\\d+)', str(filename))\n    return int(match.group(1)) if match else None\n\n\ndef load_eval_results(eval_path):\n    \"\"\"Load evaluation results from JSON file.\"\"\"\n    if not eval_path.exists():\n        return None\n    \n    with open(eval_path) as f:\n        data = json.load(f)\n    \n    # Add step from filename if not in checkpoint path\n    if 'step' not in data:\n        data['step'] = extract_step_from_filename(eval_path)\n    \n    return data\n\n\n# Discover and load all eval results\neval_results = {}  # exp_name -> list of (step, results)\n\nfor exp_name, config in EXPERIMENTS.items():\n    eval_files = find_eval_files(config[\"path\"])\n    \n    if not eval_files:\n        print(f\"[--] {exp_name}: No eval results\")\n        continue\n    \n    exp_results = []\n    for eval_file in sorted(eval_files):\n        result = load_eval_results(eval_file)\n        if result:\n            step = extract_step_from_filename(eval_file)\n            pass1 = result['results'].get('ARC/pass@1', 0) * 100\n            exp_results.append({\n                'step': step,\n                'file': eval_file.name,\n                'results': result['results'],\n            })\n            print(f\"[OK] {exp_name}: step={step:,}, pass@1={pass1:.2f}%\")\n    \n    if exp_results:\n        eval_results[exp_name] = exp_results\n\nprint(f\"\\nLoaded results for {len(eval_results)}/{len(EXPERIMENTS)} experiments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Duration Analysis\n",
    "\n",
    "**Critical for fair comparison**: Calculate actual training duration (data passes) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate training duration for each experiment checkpoint\ntraining_info = []\n\nfor exp_name, config in EXPERIMENTS.items():\n    if exp_name not in eval_results:\n        row = {\n            \"Model\": exp_name,\n            \"Batch Size\": config[\"batch_size\"],\n            \"Steps\": np.nan,\n            \"Samples Seen\": np.nan,\n            \"Data Passes\": np.nan,\n        }\n        training_info.append(row)\n        continue\n    \n    for checkpoint in eval_results[exp_name]:\n        step = checkpoint['step']\n        samples_seen = step * config[\"batch_size\"]\n        data_passes = samples_seen / config[\"dataset_samples\"]\n        \n        row = {\n            \"Model\": exp_name,\n            \"Batch Size\": config[\"batch_size\"],\n            \"Steps\": step,\n            \"Samples Seen\": samples_seen,\n            \"Data Passes\": data_passes,\n        }\n        training_info.append(row)\n\ndf_training = pd.DataFrame(training_info)\n\nprint(\"=\" * 80)\nprint(\"TRAINING DURATION COMPARISON\")\nprint(\"=\" * 80)\nprint(\"\\nData Passes = how many times the model saw each training sample on average\")\nprint()\ndf_training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show step comparison for fair analysis\nprint(\"=\" * 80)\nprint(\"STEP COMPARISON FOR FAIR ANALYSIS\")\nprint(\"=\" * 80)\n\n# Group by model and show all checkpoints\nfor exp_name in EXPERIMENTS.keys():\n    exp_data = df_training[df_training['Model'] == exp_name]\n    if exp_data['Steps'].isna().all():\n        print(f\"\\n{exp_name}: No checkpoints\")\n    else:\n        print(f\"\\n{exp_name}:\")\n        for _, row in exp_data.iterrows():\n            print(f\"  Step {row['Steps']:,.0f} → {row['Data Passes']:.1f} data passes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Training Metrics from W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch final runs from W&B for train accuracy\n",
    "df_wandb = fetch_final_runs()\n",
    "\n",
    "# Map W&B run names to our experiment names\n",
    "wandb_mapping = {\n",
    "    \"F1_standard\": \"F1: Standard\",\n",
    "    \"F2_hybrid_var\": \"F2: Hybrid VAE\",\n",
    "    \"F3_etrmtrm\": \"F3: ETRMTRM\",\n",
    "    \"F4_lpn_var\": \"F4: LPN VAE\",\n",
    "}\n",
    "\n",
    "# Extract train accuracy\n",
    "train_acc = {}\n",
    "for _, row in df_wandb.iterrows():\n",
    "    if row['display_name'] in wandb_mapping:\n",
    "        exp_name = wandb_mapping[row['display_name']]\n",
    "        train_acc[exp_name] = row['train_exact_acc']\n",
    "\n",
    "# Original TRM train accuracy (from paper - nearly 100%)\n",
    "train_acc[\"TRM (Original)\"] = 99.9\n",
    "\n",
    "print(\"Training accuracies:\")\n",
    "for name, acc in sorted(train_acc.items()):\n",
    "    print(f\"  {name}: {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Main Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build comprehensive results table (one row per checkpoint)\nrows = []\n\nfor exp_name, config in EXPERIMENTS.items():\n    if exp_name not in eval_results:\n        row = {\n            \"Model\": exp_name,\n            \"Encoder\": config[\"encoder_type\"],\n            \"Steps\": np.nan,\n            \"Data Passes\": np.nan,\n            \"Train EM%\": train_acc.get(exp_name, np.nan),\n            \"Pass@1%\": np.nan,\n            \"Pass@2%\": np.nan,\n            \"Pass@5%\": np.nan,\n        }\n        rows.append(row)\n        continue\n    \n    for checkpoint in eval_results[exp_name]:\n        step = checkpoint['step']\n        samples_seen = step * config[\"batch_size\"]\n        data_passes = samples_seen / config[\"dataset_samples\"]\n        results = checkpoint['results']\n        \n        row = {\n            \"Model\": exp_name,\n            \"Encoder\": config[\"encoder_type\"],\n            \"Steps\": step,\n            \"Data Passes\": data_passes,\n            \"Train EM%\": train_acc.get(exp_name, np.nan),\n            \"Pass@1%\": results.get(\"ARC/pass@1\", 0) * 100,\n            \"Pass@2%\": results.get(\"ARC/pass@2\", 0) * 100,\n            \"Pass@5%\": results.get(\"ARC/pass@5\", 0) * 100,\n        }\n        rows.append(row)\n\ndf_results = pd.DataFrame(rows)\ndf_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Paper-Ready Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create paper-ready table\npaper_table = df_results[[\n    \"Model\", \"Encoder\", \"Steps\", \"Data Passes\", \"Train EM%\", \"Pass@1%\", \"Pass@2%\", \"Pass@5%\"\n]].copy()\n\n# Format columns\npaper_table[\"Steps\"] = paper_table[\"Steps\"].apply(\n    lambda x: f\"{x:,.0f}\" if not pd.isna(x) else \"-\"\n)\npaper_table[\"Data Passes\"] = paper_table[\"Data Passes\"].apply(\n    lambda x: f\"{x:.1f}\" if not pd.isna(x) else \"-\"\n)\nfor col in [\"Train EM%\", \"Pass@1%\", \"Pass@2%\", \"Pass@5%\"]:\n    paper_table[col] = paper_table[col].apply(\n        lambda x: f\"{x:.2f}\" if not pd.isna(x) else \"-\"\n    )\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MAIN RESULTS TABLE (all checkpoints)\")\nprint(\"=\"*80)\nprint(dataframe_to_markdown(paper_table))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build markdown content for export\nresults_content = f\"\"\"# Evaluation Results Comparison\n\nEvaluation on 32 held-out test puzzle groups with majority voting across augmentations.\n\n## All Results (Multiple Checkpoints)\n\n{dataframe_to_markdown(paper_table)}\n\n### Column Definitions\n\n- **Steps**: Training steps (gradient updates)\n- **Data Passes**: Number of times the model saw each training sample (steps × batch_size / dataset_size)\n- **Train EM%**: Exact match accuracy on training puzzles\n- **Pass@k%**: Test accuracy with k attempts, using majority voting across augmented versions\n\n### Notes\n\n- Evaluation uses 32 held-out puzzle groups (true generalization test)\n- Original TRM uses puzzle-specific learned embeddings (cannot generalize to new puzzles)\n- ETRM variants use encoder networks that can potentially generalize to unseen puzzles\n- TRM has two checkpoints to enable fair comparison at different training stages\n\"\"\"\n\nsave_table(results_content, \"eval_results_comparison\")\nprint(\"\\nSaved to docs/project-report/tables/eval_results_comparison.md\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY\")\nprint(\"=\"*80)\n\n# Count unique experiments with results\nexps_with_results = len(eval_results)\ntotal_checkpoints = sum(len(v) for v in eval_results.values())\nprint(f\"\\nExperiments with results: {exps_with_results}/{len(EXPERIMENTS)}\")\nprint(f\"Total checkpoints evaluated: {total_checkpoints}\")\n\n# Find best result\nbest_pass1 = 0\nbest_model = \"\"\nbest_step = 0\nfor exp_name, checkpoints in eval_results.items():\n    for cp in checkpoints:\n        pass1 = cp['results'].get('ARC/pass@1', 0) * 100\n        if pass1 > best_pass1:\n            best_pass1 = pass1\n            best_model = exp_name\n            best_step = cp['step']\n\nprint(f\"\\nBest Pass@1: {best_model} @ step {best_step:,} ({best_pass1:.2f}%)\")\n\n# Step comparison for fairness\nprint(\"\\n\" + \"-\"*40)\nprint(\"FAIR COMPARISON BY STEPS\")\nprint(\"-\"*40)\nprint(\"\\nSimilar step counts allow fair comparison of gradient updates:\")\netrm_steps = [(name, cp['step']) for name, cps in eval_results.items() \n              if name != \"TRM (Original)\" for cp in cps]\nfor name, step in etrm_steps:\n    print(f\"  {name}: {step:,} steps\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}