{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Experiment Analysis for Report\n",
    "\n",
    "This notebook generates figures and tables for the experiments section:\n",
    "1. Encoder collapse evidence (cross-sample variance)\n",
    "2. Training curves (accuracy vs steps)\n",
    "3. Qualitative examples (predictions vs ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# W&B settings\n",
    "ENTITY = 'bdsaglam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Training Curves\n",
    "\n",
    "Fetch training accuracy over steps for F1, F2, F3 from W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run info for final experiments\n",
    "FINAL_RUNS = {\n",
    "    'F1_standard': {'project': 'etrm-final', 'run_id': 'z31hae14', 'label': 'ETRM-Deterministic', 'color': '#1f77b4'},\n",
    "    'F2_hybrid_var': {'project': 'etrm-final', 'run_id': '7km7llbl', 'label': 'ETRM-Variational', 'color': '#ff7f0e'},\n",
    "    'F3_etrmtrm': {'project': 'etrm-final', 'run_id': 'wj3xu8md', 'label': 'ETRM-TRM', 'color': '#2ca02c'},\n",
    "}\n",
    "\n",
    "# TRM baseline\n",
    "TRM_RUN = {'project': 'Arc1concept-aug-1000-ACT-torch', 'run_id': '2jpjeuav', 'label': 'TRM (baseline)', 'color': '#d62728'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_history(entity: str, project: str, run_id: str, metric: str = 'train/exact_accuracy') -> pd.DataFrame:\n",
    "    \"\"\"Fetch training history from W&B.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f'{entity}/{project}/{run_id}')\n",
    "    \n",
    "    # Get history\n",
    "    history = run.history(keys=['_step', metric], samples=1000)\n",
    "    history = history[history[metric].notna()]\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Fetch histories for all runs\n",
    "histories = {}\n",
    "\n",
    "for name, info in FINAL_RUNS.items():\n",
    "    print(f\"Fetching {name}...\")\n",
    "    try:\n",
    "        histories[name] = fetch_training_history(ENTITY, info['project'], info['run_id'])\n",
    "        print(f\"  Got {len(histories[name])} data points\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Also fetch TRM for comparison\n",
    "print(f\"Fetching TRM baseline...\")\n",
    "try:\n",
    "    histories['TRM'] = fetch_training_history(ENTITY, TRM_RUN['project'], TRM_RUN['run_id'])\n",
    "    print(f\"  Got {len(histories['TRM'])} data points\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot ETRM runs\n",
    "for name, info in FINAL_RUNS.items():\n",
    "    if name in histories and not histories[name].empty:\n",
    "        df = histories[name]\n",
    "        ax.plot(df['_step'] / 1000, df['train/exact_accuracy'] * 100, \n",
    "                label=info['label'], color=info['color'], linewidth=2)\n",
    "\n",
    "# Plot TRM baseline\n",
    "if 'TRM' in histories and not histories['TRM'].empty:\n",
    "    df = histories['TRM']\n",
    "    ax.plot(df['_step'] / 1000, df['train/exact_accuracy'] * 100,\n",
    "            label=TRM_RUN['label'], color=TRM_RUN['color'], linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Training Steps (k)', fontsize=14)\n",
    "ax.set_ylabel('Training Accuracy (%)', fontsize=14)\n",
    "ax.set_title('Training Accuracy Over Time', fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=12)\n",
    "ax.set_ylim(0, 105)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Encoder Collapse Evidence\n",
    "\n",
    "To demonstrate encoder collapse, we need to:\n",
    "1. Load trained ETRM models\n",
    "2. Run inference on multiple different puzzles\n",
    "3. Capture encoder outputs\n",
    "4. Compute variance across puzzles\n",
    "\n",
    "If encoder has collapsed, variance will be very low (outputs are similar regardless of input demos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Checkpoint paths for final experiments\nCHECKPOINT_DIR = Path('../../checkpoints')\n\nCHECKPOINTS = {\n    'F1_standard': CHECKPOINT_DIR / 'etrm-final' / 'F1_standard' / 'step_174622',\n    'F2_hybrid_var': CHECKPOINT_DIR / 'etrm-final' / 'F2_hybrid_var' / 'step_174240', \n    'F3_etrmtrm': CHECKPOINT_DIR / 'etrm-final' / 'F3_etrmtrm' / 'step_87310',\n}\n\nTRM_CHECKPOINT = CHECKPOINT_DIR / 'Arc1concept-aug-1000-ACT-torch' / 'pretrain_att_arc1concept_4' / 'step_518071'\n\n# Check which checkpoints exist\nfor name, path in CHECKPOINTS.items():\n    exists = path.exists()\n    print(f\"{name}: {path} - {'EXISTS' if exists else 'NOT FOUND'}\")\n\nprint(f\"TRM: {TRM_CHECKPOINT} - {'EXISTS' if TRM_CHECKPOINT.exists() else 'NOT FOUND'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find actual checkpoint paths\n",
    "import glob\n",
    "\n",
    "checkpoint_base = Path('../../checkpoints')\n",
    "print(\"Looking for checkpoints...\")\n",
    "print()\n",
    "\n",
    "# List all checkpoint directories\n",
    "for item in sorted(checkpoint_base.glob('*')):\n",
    "    if item.is_dir():\n",
    "        print(f\"{item.name}/\")\n",
    "        for sub in sorted(item.glob('*')):\n",
    "            if sub.is_dir():\n",
    "                # Check for checkpoint files\n",
    "                ckpts = list(sub.glob('*.pt'))\n",
    "                print(f\"  {sub.name}/ ({len(ckpts)} checkpoints)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will be filled in once we identify the correct checkpoint paths\n",
    "# For now, let's define the analysis function\n",
    "\n",
    "def compute_encoder_output_stats(model, dataloader, num_samples=100, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute statistics of encoder outputs across different puzzle demos.\n",
    "    \n",
    "    Returns:\n",
    "        dict with mean, std, and per-dimension variance of encoder outputs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoder_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Move batch to device\n",
    "            demo_inputs = batch['demo_inputs'].to(device)\n",
    "            demo_labels = batch['demo_labels'].to(device)\n",
    "            demo_mask = batch['demo_mask'].to(device)\n",
    "            \n",
    "            # Get encoder output\n",
    "            # This depends on the model architecture\n",
    "            if hasattr(model, 'encoder'):\n",
    "                enc_out = model.encoder(demo_inputs, demo_labels, demo_mask)\n",
    "                encoder_outputs.append(enc_out.cpu())\n",
    "    \n",
    "    if not encoder_outputs:\n",
    "        return None\n",
    "    \n",
    "    # Stack all outputs\n",
    "    all_outputs = torch.cat(encoder_outputs, dim=0)  # [N, seq_len, hidden_dim]\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean = all_outputs.mean(dim=0)  # [seq_len, hidden_dim]\n",
    "    std = all_outputs.std(dim=0)    # [seq_len, hidden_dim]\n",
    "    \n",
    "    # Overall variance across samples\n",
    "    overall_variance = all_outputs.var(dim=0).mean().item()\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'overall_variance': overall_variance,\n",
    "        'num_samples': len(encoder_outputs),\n",
    "    }\n",
    "\n",
    "print(\"Encoder analysis function defined.\")\n",
    "print(\"To run this analysis, we need to load the trained models and a dataloader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Alternative: Analyze from W&B logged metrics\n",
    "\n",
    "If we logged encoder output statistics during training, we can fetch them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what metrics are available in the runs\n",
    "api = wandb.Api()\n",
    "\n",
    "for name, info in FINAL_RUNS.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    run = api.run(f\"{ENTITY}/{info['project']}/{info['run_id']}\")\n",
    "    \n",
    "    # Get summary keys\n",
    "    summary_keys = [k for k in run.summary.keys() if not k.startswith('_')]\n",
    "    print(f\"  Summary metrics: {summary_keys[:20]}...\")  # First 20\n",
    "    \n",
    "    # Check for encoder-related metrics\n",
    "    encoder_metrics = [k for k in summary_keys if 'encoder' in k.lower() or 'emb' in k.lower() or 'variance' in k.lower()]\n",
    "    if encoder_metrics:\n",
    "        print(f\"  Encoder-related metrics: {encoder_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Qualitative Examples\n",
    "\n",
    "Show side-by-side predictions: Input → TRM → ETRM → Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARC color palette\n",
    "ARC_COLORS = {\n",
    "    0: '#000000',  # Black\n",
    "    1: '#0074D9',  # Blue\n",
    "    2: '#FF4136',  # Red\n",
    "    3: '#2ECC40',  # Green\n",
    "    4: '#FFDC00',  # Yellow\n",
    "    5: '#AAAAAA',  # Gray\n",
    "    6: '#F012BE',  # Magenta\n",
    "    7: '#FF851B',  # Orange\n",
    "    8: '#7FDBFF',  # Cyan\n",
    "    9: '#870C25',  # Brown\n",
    "}\n",
    "\n",
    "def plot_grid(grid, ax, title=''):\n",
    "    \"\"\"Plot a single ARC grid.\"\"\"\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    cmap = ListedColormap([ARC_COLORS[i] for i in range(10)])\n",
    "    \n",
    "    ax.imshow(grid, cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(grid.shape[0] + 1):\n",
    "        ax.axhline(i - 0.5, color='white', linewidth=0.5)\n",
    "    for j in range(grid.shape[1] + 1):\n",
    "        ax.axvline(j - 0.5, color='white', linewidth=0.5)\n",
    "\n",
    "\n",
    "def plot_example(demo_inputs, demo_outputs, test_input, test_output, \n",
    "                 trm_pred=None, etrm_pred=None, title=''):\n",
    "    \"\"\"Plot a complete example with demos, test, and predictions.\"\"\"\n",
    "    \n",
    "    num_demos = len(demo_inputs)\n",
    "    num_cols = max(num_demos, 4)  # At least 4 columns for predictions\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_cols, figsize=(3 * num_cols, 9))\n",
    "    \n",
    "    # Row 1: Demo inputs\n",
    "    for i in range(num_cols):\n",
    "        if i < num_demos:\n",
    "            plot_grid(demo_inputs[i], axes[0, i], f'Demo {i+1} Input')\n",
    "        else:\n",
    "            axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: Demo outputs\n",
    "    for i in range(num_cols):\n",
    "        if i < num_demos:\n",
    "            plot_grid(demo_outputs[i], axes[1, i], f'Demo {i+1} Output')\n",
    "        else:\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    # Row 3: Test input, Ground truth, TRM pred, ETRM pred\n",
    "    plot_grid(test_input, axes[2, 0], 'Test Input')\n",
    "    plot_grid(test_output, axes[2, 1], 'Ground Truth')\n",
    "    \n",
    "    if trm_pred is not None:\n",
    "        plot_grid(trm_pred, axes[2, 2], 'TRM Prediction')\n",
    "    else:\n",
    "        axes[2, 2].axis('off')\n",
    "    \n",
    "    if etrm_pred is not None:\n",
    "        plot_grid(etrm_pred, axes[2, 3], 'ETRM Prediction')\n",
    "    else:\n",
    "        axes[2, 3].axis('off')\n",
    "    \n",
    "    for i in range(4, num_cols):\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test puzzles for qualitative analysis\n",
    "import json\n",
    "\n",
    "test_puzzles_path = Path('../../data/arc1concept-aug-1000/test_puzzles.json')\n",
    "\n",
    "if test_puzzles_path.exists():\n",
    "    with open(test_puzzles_path) as f:\n",
    "        test_puzzles = json.load(f)\n",
    "    print(f\"Loaded {len(test_puzzles)} test puzzles\")\n",
    "    \n",
    "    # Show first few puzzle names\n",
    "    puzzle_names = list(test_puzzles.keys())[:10]\n",
    "    print(f\"First 10 puzzles: {puzzle_names}\")\n",
    "else:\n",
    "    print(f\"Test puzzles not found at {test_puzzles_path}\")\n",
    "    test_puzzles = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample puzzle (without model predictions for now)\n",
    "if test_puzzles:\n",
    "    # Pick a puzzle\n",
    "    puzzle_name = puzzle_names[0]\n",
    "    puzzle = test_puzzles[puzzle_name]\n",
    "    \n",
    "    print(f\"Puzzle: {puzzle_name}\")\n",
    "    print(f\"  Demos: {len(puzzle['train'])}\")\n",
    "    print(f\"  Test cases: {len(puzzle['test'])}\")\n",
    "    \n",
    "    # Extract data\n",
    "    demo_inputs = [np.array(d['input']) for d in puzzle['train']]\n",
    "    demo_outputs = [np.array(d['output']) for d in puzzle['train']]\n",
    "    test_input = np.array(puzzle['test'][0]['input'])\n",
    "    test_output = np.array(puzzle['test'][0]['output'])\n",
    "    \n",
    "    # Plot\n",
    "    fig = plot_example(\n",
    "        demo_inputs, demo_outputs,\n",
    "        test_input, test_output,\n",
    "        title=f'Puzzle: {puzzle_name}'\n",
    "    )\n",
    "    \n",
    "    plt.savefig(OUTPUT_DIR / f'example_{puzzle_name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. **Training curves** - Saved to `outputs/training_curves.png`\n",
    "2. **Encoder collapse analysis** - Requires loading trained models (paths to be configured)\n",
    "3. **Qualitative examples** - Puzzle visualization ready, model predictions need checkpoint loading\n",
    "\n",
    "### Next Steps\n",
    "- [ ] Configure correct checkpoint paths\n",
    "- [ ] Run encoder variance analysis\n",
    "- [ ] Generate predictions for qualitative examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}